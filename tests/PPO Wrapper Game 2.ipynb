{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8df2bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random, math\n",
    "import numpy as np\n",
    "import arcade\n",
    "from skimage import data, color\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from PIL import Image\n",
    "\n",
    "        \n",
    "from LightEnvCopy import LightEnv\n",
    "\n",
    "import gym.spaces\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "# Do the math to figure out our screen dimensions\n",
    "SCREEN_WIDTH = 800\n",
    "SCREEN_HEIGHT = 600\n",
    "SCREEN_TITLE = \"Game 1: Let There Be Light!\"\n",
    "\n",
    "# COnvenient data structure to hold information about actions\n",
    "Action = namedtuple('Action', 'name index delta_i delta_j')\n",
    "\n",
    "up = Action('up', 0, -1, 0)    \n",
    "down = Action('down', 1, 1, 0)    \n",
    "left = Action('left', 2, 0, -1)    \n",
    "right = Action('right', 3, 0, 1)    \n",
    "\n",
    "index_to_actions = {}\n",
    "for action in [up, down, left, right]:\n",
    "    index_to_actions[action.index] = action\n",
    "# print(index_to_actions[0].name)\n",
    "str_to_actions = {}\n",
    "for action in [up, down, left, right]:\n",
    "    str_to_actions[action.name] = action\n",
    "#TF End - Adding in actions for action conversion\n",
    "\n",
    "\n",
    "class LightEnvWrapper(gym.Env, LightEnv):\n",
    "    \"\"\"Class that wraps the Lights Environment to make it \n",
    "    compatible with RLLib.\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"rgb_array\", \"state_pixels\"]}\n",
    "    \n",
    "    def __init__(self, config: EnvContext):\n",
    "        super().__init__(SCREEN_WIDTH, SCREEN_HEIGHT, SCREEN_TITLE)\n",
    "        self.counting = 0    \n",
    "        self.torch_collected = False\n",
    "        self.torch_collected_count = []\n",
    "        self.mygame = LightEnv\n",
    "        \n",
    "        #The action space is a choice of 4 actions: U/D/L/R.\n",
    "        self.action_space = Discrete(4)\n",
    "        \n",
    "        #The observation space is a fixed image of the current game screen\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84,84, 4), dtype=np.uint8)\n",
    "        \n",
    "        self.counting = 0\n",
    "\n",
    "    def reset(self):\n",
    "        print(\"resetting in wrapper\")\n",
    "        \n",
    "        if self.torch_collected == 1:\n",
    "            print(\"Torch was collected this episode!\")\n",
    "        else:\n",
    "            print(\"Torch was not collected this episode...\")\n",
    "        self.torch_collected_count.append(self.torch_collected)\n",
    "        print(self.torch_collected_count)\n",
    "\n",
    "        self.render(self)\n",
    "        #Resets the state of the environment for a new episode and an initial observation.\n",
    "        obs_mygame = self.mygame.reset(self)\n",
    "        \n",
    "        #Open up the resetted image to verify working correctly.\n",
    "        obs_mygame.show()\n",
    "        \n",
    "        #Convert observation to 84x84 resolution and np array for rllib.\n",
    "        obs = self.convert_observations(obs_mygame)\n",
    "        \n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        self.counting += 1\n",
    "        \n",
    "        #Making sure an action is chosen, either: 0, 1, 2, 3.\n",
    "        assert action in [0, 1, 2, 3] #0-up,1-down,2-left,3-right.\n",
    "        \n",
    "        #Convert the numeric action to a keyword: up, down, left, right.\n",
    "        actions_myenv = index_to_actions[action].name #returns a word, one of: up/down/left/right\n",
    "#         print(f\"action taken: {actions_myenv}\")\n",
    "        \n",
    "        #Update the window with on_update()\n",
    "        self.render(self)\n",
    "        \n",
    "        #Compute observation extracted from the window (800x600), with reward and done flag.\n",
    "        obs, reward, done, torch_collected = self.mygame.step(self,actions_myenv)\n",
    "        if torch_collected == True:\n",
    "            self.torch_collected = 1\n",
    "        else:\n",
    "            self.torch_collected = 0\n",
    "                    \n",
    "#         if self.counting % 33 == 0: #33 steps roughly equates to 1 second in game time\n",
    "#             print(f\"total score is {self.score} at time: {self.mygame.time_taken_reported(self)}\")\n",
    "        \n",
    "        #Convert observation to 84x84 resolution and np array for rllib.\n",
    "        obs_mygame = self.convert_observations(obs)\n",
    "        \n",
    "        #If the reward has been obtained, reset the environment and start again\n",
    "        if done == True:\n",
    "            print(f\"done is {done}, resetting environment in wrapper.\")\n",
    "            self.reset()\n",
    "        \n",
    "        return obs_mygame, reward, done, {}\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        random.seed(seed)\n",
    "\n",
    "    def convert_observations(self, obs_mygame): #resizing and converting to array for rllib processing\n",
    "        # We normalize and concatenate observations\n",
    "        obs = obs_mygame\n",
    "        obs_resized = obs.resize((84,84))\n",
    "        obsarray = np.array(obs_resized)\n",
    "        return obsarray\n",
    "    \n",
    "    def render(self, mode='state_pixels'):\n",
    "        self.mygame.on_update(self, 1/60)\n",
    "        self.mygame.on_draw(self)\n",
    "        test = self.mygame.time_taken_reported(self)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd8f4ad",
   "metadata": {},
   "source": [
    "### Now run the rllib script to train the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b83216",
   "metadata": {},
   "source": [
    "### PPO Game 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca54d9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.LightEnvWrapper'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': False, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': True, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.9, 'lr': 0.01, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 60, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 1, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\ray_option_utils.py:266: DeprecationWarning: Setting 'object_store_memory' for actors is deprecated since it doesn't actually reserve the required object store memory. Use object spilling that's enabled by default (https://docs.ray.io/en/releases-2.0.0/ray-core/objects/object-spilling.html) instead to bypass the object store memory size limitation.\n",
      "  warnings.warn(\n",
      "2022-09-21 01:54:18,837\tINFO worker.py:1518 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(pid=21924)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=21924)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=21924)\u001b[0m C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=21924)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m 2022-09-21 01:54:33,263\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m [False]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-21 01:54:38,694\tINFO trainable.py:160 -- Trainable.setup took 22.455 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-09-21 01:54:38,699\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode  0\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m [False, 0]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting\n",
      "episode reward mean:  nan\n",
      "checkpoint saved at C:\\Users\\Tim/ray_results\\PPO_LightEnvWrapper_2022-09-21_01-54-169v3hnijo\\checkpoint_000001\n",
      "End of episode  0\n",
      "Starting episode  1\n",
      "episode reward mean:  nan\n",
      "End of episode  1\n",
      "Starting episode  2\n",
      "episode reward mean:  nan\n",
      "End of episode  2\n",
      "Starting episode  3\n",
      "episode reward mean:  nan\n",
      "End of episode  3\n",
      "Starting episode  4\n",
      "episode reward mean:  nan\n",
      "End of episode  4\n",
      "Starting episode  5\n",
      "episode reward mean:  nan\n",
      "End of episode  5\n",
      "Starting episode  6\n",
      "episode reward mean:  nan\n",
      "End of episode  6\n",
      "Starting episode  7\n",
      "episode reward mean:  nan\n",
      "End of episode  7\n",
      "Starting episode  8\n",
      "episode reward mean:  nan\n",
      "End of episode  8\n",
      "Starting episode  9\n",
      "episode reward mean:  nan\n",
      "End of episode  9\n",
      "Starting episode  10\n",
      "episode reward mean:  nan\n",
      "checkpoint saved at C:\\Users\\Tim/ray_results\\PPO_LightEnvWrapper_2022-09-21_01-54-169v3hnijo\\checkpoint_000011\n",
      "End of episode  10\n",
      "Starting episode  11\n",
      "episode reward mean:  nan\n",
      "End of episode  11\n",
      "Starting episode  12\n",
      "episode reward mean:  nan\n",
      "End of episode  12\n",
      "Starting episode  13\n",
      "episode reward mean:  nan\n",
      "End of episode  13\n",
      "Starting episode  14\n",
      "episode reward mean:  nan\n",
      "End of episode  14\n",
      "Starting episode  15\n",
      "episode reward mean:  nan\n",
      "End of episode  15\n",
      "Starting episode  16\n",
      "episode reward mean:  nan\n",
      "End of episode  16\n",
      "Starting episode  17\n",
      "episode reward mean:  nan\n",
      "End of episode  17\n",
      "Starting episode  18\n",
      "episode reward mean:  nan\n",
      "End of episode  18\n",
      "Starting episode  19\n",
      "episode reward mean:  nan\n",
      "End of episode  19\n",
      "Starting episode  20\n",
      "episode reward mean:  nan\n",
      "checkpoint saved at C:\\Users\\Tim/ray_results\\PPO_LightEnvWrapper_2022-09-21_01-54-169v3hnijo\\checkpoint_000021\n",
      "End of episode  20\n",
      "Starting episode  21\n",
      "episode reward mean:  nan\n",
      "End of episode  21\n",
      "Starting episode  22\n",
      "episode reward mean:  nan\n",
      "End of episode  22\n",
      "Starting episode  23\n",
      "episode reward mean:  nan\n",
      "End of episode  23\n",
      "Starting episode  24\n",
      "episode reward mean:  nan\n",
      "End of episode  24\n",
      "Starting episode  25\n",
      "episode reward mean:  nan\n",
      "End of episode  25\n",
      "Starting episode  26\n",
      "episode reward mean:  nan\n",
      "End of episode  26\n",
      "Starting episode  27\n",
      "episode reward mean:  nan\n",
      "End of episode  27\n",
      "Starting episode  28\n",
      "episode reward mean:  nan\n",
      "End of episode  28\n",
      "Starting episode  29\n",
      "episode reward mean:  nan\n",
      "End of episode  29\n",
      "Starting episode  30\n",
      "episode reward mean:  nan\n",
      "checkpoint saved at C:\\Users\\Tim/ray_results\\PPO_LightEnvWrapper_2022-09-21_01-54-169v3hnijo\\checkpoint_000031\n",
      "End of episode  30\n",
      "Starting episode  31\n",
      "episode reward mean:  nan\n",
      "End of episode  31\n",
      "Starting episode  32\n",
      "episode reward mean:  nan\n",
      "End of episode  32\n",
      "Starting episode  33\n",
      "episode reward mean:  nan\n",
      "End of episode  33\n",
      "Starting episode  34\n",
      "episode reward mean:  nan\n",
      "End of episode  34\n",
      "Starting episode  35\n",
      "episode reward mean:  nan\n",
      "End of episode  35\n",
      "Starting episode  36\n",
      "episode reward mean:  nan\n",
      "End of episode  36\n",
      "Starting episode  37\n",
      "episode reward mean:  nan\n",
      "End of episode  37\n",
      "Starting episode  38\n",
      "episode reward mean:  nan\n",
      "End of episode  38\n",
      "Starting episode  39\n",
      "episode reward mean:  nan\n",
      "End of episode  39\n",
      "Starting episode  40\n",
      "episode reward mean:  nan\n",
      "checkpoint saved at C:\\Users\\Tim/ray_results\\PPO_LightEnvWrapper_2022-09-21_01-54-169v3hnijo\\checkpoint_000041\n",
      "End of episode  40\n",
      "Starting episode  41\n",
      "episode reward mean:  nan\n",
      "End of episode  41\n",
      "Starting episode  42\n",
      "episode reward mean:  nan\n",
      "End of episode  42\n",
      "Starting episode  43\n",
      "episode reward mean:  nan\n",
      "End of episode  43\n",
      "Starting episode  44\n",
      "episode reward mean:  nan\n",
      "End of episode  44\n",
      "Starting episode  45\n",
      "episode reward mean:  nan\n",
      "End of episode  45\n",
      "Starting episode  46\n",
      "episode reward mean:  nan\n",
      "End of episode  46\n",
      "Starting episode  47\n",
      "episode reward mean:  nan\n",
      "End of episode  47\n",
      "Starting episode  48\n",
      "episode reward mean:  nan\n",
      "End of episode  48\n",
      "Starting episode  49\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m Game completed with a score of: -3193 at time: 3268\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m done is True, resetting environment in wrapper.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m Torch was collected this episode!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m [False, 0, 1]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m [False, 0, 1, False]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting\n",
      "episode reward mean:  -3193.0\n",
      "End of episode  49\n",
      "Starting episode  50\n",
      "episode reward mean:  -3193.0\n",
      "checkpoint saved at C:\\Users\\Tim/ray_results\\PPO_LightEnvWrapper_2022-09-21_01-54-169v3hnijo\\checkpoint_000051\n",
      "End of episode  50\n",
      "Starting episode  51\n",
      "episode reward mean:  -3193.0\n",
      "End of episode  51\n",
      "Starting episode  52\n",
      "episode reward mean:  -3193.0\n",
      "End of episode  52\n",
      "Starting episode  53\n",
      "episode reward mean:  -3193.0\n",
      "End of episode  53\n",
      "Starting episode  54\n",
      "episode reward mean:  -3193.0\n",
      "End of episode  54\n",
      "Starting episode  55\n",
      "episode reward mean:  -3193.0\n",
      "End of episode  55\n",
      "Starting episode  56\n",
      "episode reward mean:  -3193.0\n",
      "End of episode  56\n",
      "Starting episode  57\n",
      "episode reward mean:  -3193.0\n",
      "End of episode  57\n",
      "Starting episode  58\n",
      "episode reward mean:  -3193.0\n",
      "End of episode  58\n",
      "Starting episode  59\n",
      "episode reward mean:  -3193.0\n",
      "End of episode  59\n",
      "Starting episode  60\n",
      "episode reward mean:  -3193.0\n",
      "checkpoint saved at C:\\Users\\Tim/ray_results\\PPO_LightEnvWrapper_2022-09-21_01-54-169v3hnijo\\checkpoint_000061\n",
      "End of episode  60\n",
      "Starting episode  61\n",
      "episode reward mean:  -3193.0\n",
      "End of episode  61\n",
      "Starting episode  62\n",
      "episode reward mean:  -3193.0\n",
      "End of episode  62\n",
      "Starting episode  63\n",
      "episode reward mean:  -3193.0\n",
      "End of episode  63\n",
      "Starting episode  64\n",
      "episode reward mean:  -3193.0\n",
      "End of episode  64\n",
      "Starting episode  65\n",
      "episode reward mean:  -3193.0\n",
      "End of episode  65\n",
      "Starting episode  66\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m Game completed with a score of: -1081 at time: 1158\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m done is True, resetting environment in wrapper.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m Torch was collected this episode!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m [False, 0, 1, False, 1]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m [False, 0, 1, False, 1, False]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting\n",
      "episode reward mean:  -2137.0\n",
      "End of episode  66\n",
      "Starting episode  67\n",
      "episode reward mean:  -2137.0\n",
      "End of episode  67\n",
      "Starting episode  68\n",
      "episode reward mean:  -2137.0\n",
      "End of episode  68\n",
      "Starting episode  69\n",
      "episode reward mean:  -2137.0\n",
      "End of episode  69\n",
      "Starting episode  70\n",
      "episode reward mean:  -2137.0\n",
      "checkpoint saved at C:\\Users\\Tim/ray_results\\PPO_LightEnvWrapper_2022-09-21_01-54-169v3hnijo\\checkpoint_000071\n",
      "End of episode  70\n",
      "Starting episode  71\n",
      "episode reward mean:  -2137.0\n",
      "End of episode  71\n",
      "Starting episode  72\n",
      "episode reward mean:  -2137.0\n",
      "End of episode  72\n",
      "Starting episode  73\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m Game completed with a score of: -394 at time: 475\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m done is True, resetting environment in wrapper.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m Torch was collected this episode!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m [False, 0, 1, False, 1, False, 1]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m [False, 0, 1, False, 1, False, 1, False]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting\n",
      "episode reward mean:  -1556.0\n",
      "End of episode  73\n",
      "Starting episode  74\n",
      "episode reward mean:  -1556.0\n",
      "End of episode  74\n",
      "Starting episode  75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode reward mean:  -1556.0\n",
      "End of episode  75\n",
      "Starting episode  76\n",
      "episode reward mean:  -1556.0\n",
      "End of episode  76\n",
      "Starting episode  77\n",
      "episode reward mean:  -1556.0\n",
      "End of episode  77\n",
      "Starting episode  78\n",
      "episode reward mean:  -1556.0\n",
      "End of episode  78\n",
      "Starting episode  79\n",
      "episode reward mean:  -1556.0\n",
      "End of episode  79\n",
      "Starting episode  80\n",
      "episode reward mean:  -1556.0\n",
      "checkpoint saved at C:\\Users\\Tim/ray_results\\PPO_LightEnvWrapper_2022-09-21_01-54-169v3hnijo\\checkpoint_000081\n",
      "End of episode  80\n",
      "Starting episode  81\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m Game completed with a score of: -424 at time: 499\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m done is True, resetting environment in wrapper.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m Torch was collected this episode!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m [False, 0, 1, False, 1, False, 1, False, 1]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m [False, 0, 1, False, 1, False, 1, False, 1, False]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting\n",
      "episode reward mean:  -1273.0\n",
      "End of episode  81\n",
      "Starting episode  82\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m Game completed with a score of: 13 at time: 77\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m done is True, resetting environment in wrapper.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m [False, 0, 1, False, 1, False, 1, False, 1, False, 0]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m [False, 0, 1, False, 1, False, 1, False, 1, False, 0, False]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21924)\u001b[0m resetting\n",
      "episode reward mean:  -1015.8\n",
      "End of episode  82\n",
      "Starting episode  83\n",
      "episode reward mean:  -1015.8\n",
      "End of episode  83\n",
      "Starting episode  84\n",
      "episode reward mean:  -1015.8\n",
      "End of episode  84\n",
      "Starting episode  85\n",
      "episode reward mean:  -1015.8\n",
      "End of episode  85\n",
      "Starting episode  86\n",
      "episode reward mean:  -1015.8\n",
      "End of episode  86\n",
      "Starting episode  87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-21 15:57:22,114\tERROR algorithm.py:2173 -- Error in training or evaluation attempt! Trying to recover.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 2373, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo.py\", line 407, in training_step\n",
      "    train_batch = synchronous_parallel_sample(\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\execution\\rollout_ops.py\", line 100, in synchronous_parallel_sample\n",
      "    sample_batches = ray.get(\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\worker.py\", line 2275, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RayOutOfMemoryError): \u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=21924, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002140FA8DF70>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 620, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\memory_monitor.py\", line 162, in raise_if_low_memory\n",
      "    raise RayOutOfMemoryError(\n",
      "ray._private.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node DESKTOP-BKAPO4O is used (7.43 / 7.73 GB). The top 10 memory consumers are:\n",
      "\n",
      "PID\tMEM\tCOMMAND\n",
      "4340\t0.55GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe -m ipykernel_launcher -f C:\\Users\\Tim\\AppData\\Roaming\\j\n",
      "20976\t0.48GiB\tC:\\Users\\Tim\\AppData\\Local\\Microsoft\\OneDrive\\OneDrive.exe /background\n",
      "12768\t0.45GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "22592\t0.38GiB\tC:\\WINDOWS\\SystemApps\\Microsoft.Windows.Search_cw5n1h2txyewy\\SearchApp.exe -ServerName:ShellFeedsUI.\n",
      "21924\t0.38GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "21312\t0.12GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=gpu-process --gpu-preferences=UA\n",
      "17220\t0.1GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --no-startup-window /prefetch:5\n",
      "5688\t0.09GiB\tC:\\Users\\Tim\\AppData\\Roaming\\Spotify\\Spotify.exe --type=renderer --log-severity=disable --user-agent\n",
      "19596\t0.08GiB\tC:\\Users\\Tim\\AppData\\Roaming\\Spotify\\Spotify.exe --autostart --minimized\n",
      "21820\t0.08GiB\tC:\\WINDOWS\\Explorer.EXE\n",
      "\n",
      "In addition, up to 0.0 GiB of shared memory is currently being used by the Ray object store.\n",
      "---\n",
      "--- Tip: Use the `ray memory` command to list active objects in the cluster.\n",
      "--- To disable OOM exceptions, set RAY_DISABLE_MEMORY_MONITOR=1.\n",
      "---\n",
      "2022-09-21 15:57:23,735\tERROR worker_set.py:728 -- Worker 1 is faulty.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 2373, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo.py\", line 407, in training_step\n",
      "    train_batch = synchronous_parallel_sample(\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\execution\\rollout_ops.py\", line 100, in synchronous_parallel_sample\n",
      "    sample_batches = ray.get(\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\worker.py\", line 2275, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RayOutOfMemoryError): \u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=21924, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002140FA8DF70>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 620, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\memory_monitor.py\", line 162, in raise_if_low_memory\n",
      "    raise RayOutOfMemoryError(\n",
      "ray._private.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node DESKTOP-BKAPO4O is used (7.43 / 7.73 GB). The top 10 memory consumers are:\n",
      "\n",
      "PID\tMEM\tCOMMAND\n",
      "4340\t0.55GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe -m ipykernel_launcher -f C:\\Users\\Tim\\AppData\\Roaming\\j\n",
      "20976\t0.48GiB\tC:\\Users\\Tim\\AppData\\Local\\Microsoft\\OneDrive\\OneDrive.exe /background\n",
      "12768\t0.45GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "22592\t0.38GiB\tC:\\WINDOWS\\SystemApps\\Microsoft.Windows.Search_cw5n1h2txyewy\\SearchApp.exe -ServerName:ShellFeedsUI.\n",
      "21924\t0.38GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "21312\t0.12GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=gpu-process --gpu-preferences=UA\n",
      "17220\t0.1GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --no-startup-window /prefetch:5\n",
      "5688\t0.09GiB\tC:\\Users\\Tim\\AppData\\Roaming\\Spotify\\Spotify.exe --type=renderer --log-severity=disable --user-agent\n",
      "19596\t0.08GiB\tC:\\Users\\Tim\\AppData\\Roaming\\Spotify\\Spotify.exe --autostart --minimized\n",
      "21820\t0.08GiB\tC:\\WINDOWS\\Explorer.EXE\n",
      "\n",
      "In addition, up to 0.0 GiB of shared memory is currently being used by the Ray object store.\n",
      "---\n",
      "--- Tip: Use the `ray memory` command to list active objects in the cluster.\n",
      "--- To disable OOM exceptions, set RAY_DISABLE_MEMORY_MONITOR=1.\n",
      "---\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 725, in _worker_health_check\n",
      "    ray.get(obj_ref)\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\worker.py\", line 2275, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RayOutOfMemoryError): \u001b[36mray::RolloutWorker.sample_with_count()\u001b[39m (pid=21924, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002140FA8DF70>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 620, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\memory_monitor.py\", line 162, in raise_if_low_memory\n",
      "    raise RayOutOfMemoryError(\n",
      "ray._private.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node DESKTOP-BKAPO4O is used (7.36 / 7.73 GB). The top 10 memory consumers are:\n",
      "\n",
      "PID\tMEM\tCOMMAND\n",
      "4340\t0.55GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe -m ipykernel_launcher -f C:\\Users\\Tim\\AppData\\Roaming\\j\n",
      "20976\t0.48GiB\tC:\\Users\\Tim\\AppData\\Local\\Microsoft\\OneDrive\\OneDrive.exe /background\n",
      "12768\t0.45GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "21924\t0.38GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "22592\t0.37GiB\tC:\\WINDOWS\\SystemApps\\Microsoft.Windows.Search_cw5n1h2txyewy\\SearchApp.exe -ServerName:ShellFeedsUI.\n",
      "17220\t0.12GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --no-startup-window /prefetch:5\n",
      "21312\t0.12GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=gpu-process --gpu-preferences=UA\n",
      "5688\t0.09GiB\tC:\\Users\\Tim\\AppData\\Roaming\\Spotify\\Spotify.exe --type=renderer --log-severity=disable --user-agent\n",
      "21820\t0.08GiB\tC:\\WINDOWS\\Explorer.EXE\n",
      "15260\t0.07GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "\n",
      "In addition, up to 0.0 GiB of shared memory is currently being used by the Ray object store.\n",
      "---\n",
      "--- Tip: Use the `ray memory` command to list active objects in the cluster.\n",
      "--- To disable OOM exceptions, set RAY_DISABLE_MEMORY_MONITOR=1.\n",
      "---\n",
      "2022-09-21 15:57:23,746\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.sample_with_count()\u001b[39m (pid=21924, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002140FA8DF70>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 620, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\memory_monitor.py\", line 162, in raise_if_low_memory\n",
      "    raise RayOutOfMemoryError(\n",
      "ray._private.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node DESKTOP-BKAPO4O is used (7.36 / 7.73 GB). The top 10 memory consumers are:\n",
      "\n",
      "PID\tMEM\tCOMMAND\n",
      "4340\t0.55GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe -m ipykernel_launcher -f C:\\Users\\Tim\\AppData\\Roaming\\j\n",
      "20976\t0.48GiB\tC:\\Users\\Tim\\AppData\\Local\\Microsoft\\OneDrive\\OneDrive.exe /background\n",
      "12768\t0.45GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "21924\t0.38GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "22592\t0.37GiB\tC:\\WINDOWS\\SystemApps\\Microsoft.Windows.Search_cw5n1h2txyewy\\SearchApp.exe -ServerName:ShellFeedsUI.\n",
      "17220\t0.12GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --no-startup-window /prefetch:5\n",
      "21312\t0.12GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=gpu-process --gpu-preferences=UA\n",
      "5688\t0.09GiB\tC:\\Users\\Tim\\AppData\\Roaming\\Spotify\\Spotify.exe --type=renderer --log-severity=disable --user-agent\n",
      "21820\t0.08GiB\tC:\\WINDOWS\\Explorer.EXE\n",
      "15260\t0.07GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "\n",
      "In addition, up to 0.0 GiB of shared memory is currently being used by the Ray object store.\n",
      "---\n",
      "--- Tip: Use the `ray memory` command to list active objects in the cluster.\n",
      "--- To disable OOM exceptions, set RAY_DISABLE_MEMORY_MONITOR=1.\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=148)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=148)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=148)\u001b[0m C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=148)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=148)\u001b[0m 2022-09-21 15:57:42,826\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=148)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=148)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=148)\u001b[0m [False]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=148)\u001b[0m resetting\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=148)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=148)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=148)\u001b[0m [False, 0]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=148)\u001b[0m resetting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-21 15:58:12,143\tERROR algorithm.py:2173 -- Error in training or evaluation attempt! Trying to recover.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 2373, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo.py\", line 407, in training_step\n",
      "    train_batch = synchronous_parallel_sample(\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\execution\\rollout_ops.py\", line 100, in synchronous_parallel_sample\n",
      "    sample_batches = ray.get(\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\worker.py\", line 2275, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RayOutOfMemoryError): \u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=148, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000021FF21DDF70>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 620, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\memory_monitor.py\", line 162, in raise_if_low_memory\n",
      "    raise RayOutOfMemoryError(\n",
      "ray._private.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node DESKTOP-BKAPO4O is used (7.36 / 7.73 GB). The top 10 memory consumers are:\n",
      "\n",
      "PID\tMEM\tCOMMAND\n",
      "148\t0.77GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "20976\t0.46GiB\tC:\\Users\\Tim\\AppData\\Local\\Microsoft\\OneDrive\\OneDrive.exe /background\n",
      "12768\t0.44GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "4340\t0.3GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe -m ipykernel_launcher -f C:\\Users\\Tim\\AppData\\Roaming\\j\n",
      "21312\t0.26GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=gpu-process --gpu-preferences=UA\n",
      "17220\t0.19GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --no-startup-window /prefetch:5\n",
      "15260\t0.13GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "16288\t0.11GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --instant-process --dis\n",
      "2500\t0.11GiB\tC:\\Program Files\\WindowsApps\\Microsoft.Windows.Photos_2022.30070.26007.0_x64__8wekyb3d8bbwe\\Microsof\n",
      "21820\t0.09GiB\tC:\\WINDOWS\\Explorer.EXE\n",
      "\n",
      "In addition, up to 0.0 GiB of shared memory is currently being used by the Ray object store.\n",
      "---\n",
      "--- Tip: Use the `ray memory` command to list active objects in the cluster.\n",
      "--- To disable OOM exceptions, set RAY_DISABLE_MEMORY_MONITOR=1.\n",
      "---\n",
      "2022-09-21 15:58:13,371\tERROR worker_set.py:728 -- Worker 1 is faulty.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 2373, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo.py\", line 407, in training_step\n",
      "    train_batch = synchronous_parallel_sample(\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\execution\\rollout_ops.py\", line 100, in synchronous_parallel_sample\n",
      "    sample_batches = ray.get(\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\worker.py\", line 2275, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RayOutOfMemoryError): \u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=148, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000021FF21DDF70>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 620, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\memory_monitor.py\", line 162, in raise_if_low_memory\n",
      "    raise RayOutOfMemoryError(\n",
      "ray._private.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node DESKTOP-BKAPO4O is used (7.36 / 7.73 GB). The top 10 memory consumers are:\n",
      "\n",
      "PID\tMEM\tCOMMAND\n",
      "148\t0.77GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "20976\t0.46GiB\tC:\\Users\\Tim\\AppData\\Local\\Microsoft\\OneDrive\\OneDrive.exe /background\n",
      "12768\t0.44GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "4340\t0.3GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe -m ipykernel_launcher -f C:\\Users\\Tim\\AppData\\Roaming\\j\n",
      "21312\t0.26GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=gpu-process --gpu-preferences=UA\n",
      "17220\t0.19GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --no-startup-window /prefetch:5\n",
      "15260\t0.13GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "16288\t0.11GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --instant-process --dis\n",
      "2500\t0.11GiB\tC:\\Program Files\\WindowsApps\\Microsoft.Windows.Photos_2022.30070.26007.0_x64__8wekyb3d8bbwe\\Microsof\n",
      "21820\t0.09GiB\tC:\\WINDOWS\\Explorer.EXE\n",
      "\n",
      "In addition, up to 0.0 GiB of shared memory is currently being used by the Ray object store.\n",
      "---\n",
      "--- Tip: Use the `ray memory` command to list active objects in the cluster.\n",
      "--- To disable OOM exceptions, set RAY_DISABLE_MEMORY_MONITOR=1.\n",
      "---\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 725, in _worker_health_check\n",
      "    ray.get(obj_ref)\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\worker.py\", line 2275, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RayOutOfMemoryError): \u001b[36mray::RolloutWorker.sample_with_count()\u001b[39m (pid=148, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000021FF21DDF70>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 620, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\memory_monitor.py\", line 162, in raise_if_low_memory\n",
      "    raise RayOutOfMemoryError(\n",
      "ray._private.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node DESKTOP-BKAPO4O is used (7.37 / 7.73 GB). The top 10 memory consumers are:\n",
      "\n",
      "PID\tMEM\tCOMMAND\n",
      "148\t0.77GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "20976\t0.46GiB\tC:\\Users\\Tim\\AppData\\Local\\Microsoft\\OneDrive\\OneDrive.exe /background\n",
      "12768\t0.43GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "4340\t0.28GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe -m ipykernel_launcher -f C:\\Users\\Tim\\AppData\\Roaming\\j\n",
      "21312\t0.24GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=gpu-process --gpu-preferences=UA\n",
      "17220\t0.19GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --no-startup-window /prefetch:5\n",
      "15260\t0.13GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "2500\t0.11GiB\tC:\\Program Files\\WindowsApps\\Microsoft.Windows.Photos_2022.30070.26007.0_x64__8wekyb3d8bbwe\\Microsof\n",
      "21820\t0.09GiB\tC:\\WINDOWS\\Explorer.EXE\n",
      "5688\t0.09GiB\tC:\\Users\\Tim\\AppData\\Roaming\\Spotify\\Spotify.exe --type=renderer --log-severity=disable --user-agent\n",
      "\n",
      "In addition, up to 0.0 GiB of shared memory is currently being used by the Ray object store.\n",
      "---\n",
      "--- Tip: Use the `ray memory` command to list active objects in the cluster.\n",
      "--- To disable OOM exceptions, set RAY_DISABLE_MEMORY_MONITOR=1.\n",
      "---\n",
      "2022-09-21 15:58:13,376\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.sample_with_count()\u001b[39m (pid=148, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000021FF21DDF70>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 620, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\memory_monitor.py\", line 162, in raise_if_low_memory\n",
      "    raise RayOutOfMemoryError(\n",
      "ray._private.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node DESKTOP-BKAPO4O is used (7.37 / 7.73 GB). The top 10 memory consumers are:\n",
      "\n",
      "PID\tMEM\tCOMMAND\n",
      "148\t0.77GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "20976\t0.46GiB\tC:\\Users\\Tim\\AppData\\Local\\Microsoft\\OneDrive\\OneDrive.exe /background\n",
      "12768\t0.43GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "4340\t0.28GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe -m ipykernel_launcher -f C:\\Users\\Tim\\AppData\\Roaming\\j\n",
      "21312\t0.24GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=gpu-process --gpu-preferences=UA\n",
      "17220\t0.19GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --no-startup-window /prefetch:5\n",
      "15260\t0.13GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "2500\t0.11GiB\tC:\\Program Files\\WindowsApps\\Microsoft.Windows.Photos_2022.30070.26007.0_x64__8wekyb3d8bbwe\\Microsof\n",
      "21820\t0.09GiB\tC:\\WINDOWS\\Explorer.EXE\n",
      "5688\t0.09GiB\tC:\\Users\\Tim\\AppData\\Roaming\\Spotify\\Spotify.exe --type=renderer --log-severity=disable --user-agent\n",
      "\n",
      "In addition, up to 0.0 GiB of shared memory is currently being used by the Ray object store.\n",
      "---\n",
      "--- Tip: Use the `ray memory` command to list active objects in the cluster.\n",
      "--- To disable OOM exceptions, set RAY_DISABLE_MEMORY_MONITOR=1.\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=20052)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=20052)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=20052)\u001b[0m C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=20052)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m 2022-09-21 15:58:25,249\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m [False]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m resetting\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m [False, 0]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m resetting\n",
      "episode reward mean:  -1015.8\n",
      "End of episode  87\n",
      "Starting episode  88\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m Game completed with a score of: -24 at time: 113\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m done is True, resetting environment in wrapper.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m Torch was collected this episode!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m [False, 0, 1]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m resetting\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m [False, 0, 1, False]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m resetting\n",
      "episode reward mean:  -850.5\n",
      "End of episode  88\n",
      "Starting episode  89\n",
      "episode reward mean:  -850.5\n",
      "End of episode  89\n",
      "Starting episode  90\n",
      "episode reward mean:  -850.5\n",
      "checkpoint saved at C:\\Users\\Tim/ray_results\\PPO_LightEnvWrapper_2022-09-21_01-54-169v3hnijo\\checkpoint_000091\n",
      "End of episode  90\n",
      "Starting episode  91\n",
      "episode reward mean:  -850.5\n",
      "End of episode  91\n",
      "Starting episode  92\n",
      "episode reward mean:  -850.5\n",
      "End of episode  92\n",
      "Starting episode  93\n",
      "episode reward mean:  -850.5\n",
      "End of episode  93\n",
      "Starting episode  94\n",
      "episode reward mean:  -850.5\n",
      "End of episode  94\n",
      "Starting episode  95\n",
      "episode reward mean:  -850.5\n",
      "End of episode  95\n",
      "Starting episode  96\n",
      "episode reward mean:  -850.5\n",
      "End of episode  96\n",
      "Starting episode  97\n",
      "episode reward mean:  -850.5\n",
      "End of episode  97\n",
      "Starting episode  98\n",
      "episode reward mean:  -850.5\n",
      "End of episode  98\n",
      "Starting episode  99\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m Game completed with a score of: -634 at time: 712\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m done is True, resetting environment in wrapper.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m Torch was collected this episode!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m [False, 0, 1, False, 1]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m resetting\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m [False, 0, 1, False, 1, False]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20052)\u001b[0m resetting\n",
      "episode reward mean:  -819.5714285714286\n",
      "End of episode  99\n",
      "Starting episode  100\n",
      "episode reward mean:  -819.5714285714286\n",
      "checkpoint saved at C:\\Users\\Tim/ray_results\\PPO_LightEnvWrapper_2022-09-21_01-54-169v3hnijo\\checkpoint_000101\n",
      "End of episode  100\n",
      "Starting episode  101\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting episode \u001b[39m\u001b[38;5;124m\"\u001b[39m, episode)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Perform one iteration of training the policy with PPO\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#print(pretty_print(result))\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode reward mean: \u001b[39m\u001b[38;5;124m\"\u001b[39m, result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_reward_mean\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py:347\u001b[0m, in \u001b[0;36mTrainable.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[0;32m    346\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 347\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:661\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    653\u001b[0m     (\n\u001b[0;32m    654\u001b[0m         results,\n\u001b[0;32m    655\u001b[0m         train_iter_ctx,\n\u001b[0;32m    656\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[0;32m    657\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 661\u001b[0m     results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_parallel_to_training\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:2373\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2371\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[0;32m   2372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_disable_execution_plan_api\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m-> 2373\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2374\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2375\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo.py:407\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    403\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m synchronous_parallel_sample(\n\u001b[0;32m    404\u001b[0m         worker_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers, max_agent_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    405\u001b[0m     )\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 407\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_batch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39mas_multi_agent()\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39magent_steps()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\execution\\rollout_ops.py:100\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[1;34m(worker_set, max_agent_steps, max_env_steps, concat)\u001b[0m\n\u001b[0;32m     97\u001b[0m     sample_batches \u001b[38;5;241m=\u001b[39m [worker_set\u001b[38;5;241m.\u001b[39mlocal_worker()\u001b[38;5;241m.\u001b[39msample()]\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 100\u001b[0m     sample_batches \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mworker\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Update our counters for the stopping criterion of the while loop.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m sample_batches:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\worker.py:2269\u001b[0m, in \u001b[0;36mget\u001b[1;34m(object_refs, timeout)\u001b[0m\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_refs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must either be an object ref \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a list of object refs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2266\u001b[0m     )\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[1;32m-> 2269\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[0;32m   2271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\worker.py:669\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[1;34m(self, object_refs, timeout)\u001b[0m\n\u001b[0;32m    663\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    664\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to call `get` on the value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_ref\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    665\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich is not an ray.ObjectRef.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    666\u001b[0m         )\n\u001b[0;32m    668\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 669\u001b[0m data_metadata_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_ms\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    672\u001b[0m debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (data, metadata) \u001b[38;5;129;01min\u001b[39;00m data_metadata_pairs:\n",
      "File \u001b[1;32mpython\\ray\\_raylet.pyx:1211\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpython\\ray\\_raylet.pyx:173\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ray.rllib.agents.ppo.ppo as ppo\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = PPOConfig()\n",
    "\n",
    "#sgd minibatch changed from 128 to 256 for game2.\n",
    "config = PPOConfig().training(gamma=0.9, lr=0.01, kl_coeff=0.2, entropy_coeff=1,\n",
    "#                               entropy_coeff_schedule=[[0,1],[1000,0]],\n",
    "                             sgd_minibatch_size=128, num_sgd_iter=60)\\\n",
    "            .resources(num_gpus=0)\\\n",
    "            .rollouts(num_envs_per_worker=1, num_rollout_workers=1, recreate_failed_workers=True)\n",
    "config.normalize_actions=False\n",
    "config.env=LightEnvWrapper\n",
    "config.clip_actions=False\n",
    "print(config.to_dict())\n",
    "# Build a Algorithm object from the config\n",
    "trainer = ppo.PPOTrainer(config=config)\n",
    "\n",
    "avg_rewards = []\n",
    "num_iterations = []\n",
    "for episode in range(1000):\n",
    "    print(\"Starting episode \", episode)\n",
    "    # Perform one iteration of training the policy with PPO\n",
    "    result = trainer.train()\n",
    "    #print(pretty_print(result))\n",
    "    print(\"episode reward mean: \", result['episode_reward_mean'])\n",
    "    avg_rewards.append(result['episode_reward_mean'])\n",
    "    num_iterations.append(episode)\n",
    "    if episode % 10 == 0:\n",
    "        checkpoint = trainer.save()\n",
    "        print(\"checkpoint saved at\", checkpoint)\n",
    "    print(\"End of episode \", episode)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ebb56db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(num_iterations, avg_rewards, label=\"Game 2 PPO\")       \n",
    "plt.xlabel(\"Training iterations\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.legend(loc=4)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "rllib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
