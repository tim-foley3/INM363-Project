{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54JlYYZbB5ek",
   "metadata": {
    "id": "54JlYYZbB5ek"
   },
   "source": [
    "# Game 1 DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-Vp-BkuYZ-Qv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Vp-BkuYZ-Qv",
    "outputId": "01812157-d9a1-401e-f867-62c184c75b7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.7.14)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"ARCADE_HEADLESS\"] = \"true\"\n",
    "\n",
    "import arcade\n",
    "from arcade.experimental import Shadertoy\n",
    "from arcade.experimental.lights import Light, LightLayer\n",
    "import pygame\n",
    "from pygame import Surface\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "# Do the math to figure out our screen dimensions\n",
    "SCREEN_WIDTH = 800\n",
    "SCREEN_HEIGHT = 600\n",
    "SCREEN_TITLE = \"Game 1: Let There Be Light!\"\n",
    "\n",
    "SPRITE_SCALING = 0.25\n",
    "\n",
    "# How fast the camera pans to the player. 1.0 is instant.\n",
    "CAMERA_SPEED = 0.1\n",
    "\n",
    "PLAYER_MOVEMENT_SPEED = 7\n",
    "BOMB_COUNT = 50\n",
    "TORCH_COUNT = 1\n",
    "PLAYING_FIELD_WIDTH = 800 #1600\n",
    "PLAYING_FIELD_HEIGHT = 600 #1600\n",
    "REWARD_COUNT = 1 #TF - Add in reward\n",
    "END_GAME = False\n",
    "torch_collected = False\n",
    "\n",
    "\n",
    "class LightEnv(arcade.Window):\n",
    "\n",
    "    def __init__(self, width, height, title):\n",
    "        super().__init__(width, height, title)\n",
    "        \n",
    "        \n",
    "#         pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "#         self.img = Surface((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "\n",
    "        self.time_before_update = 0\n",
    "        self.time_after_update = 0\n",
    "\n",
    "        arcade.Window.headless = True\n",
    "\n",
    "        self.frame = 0\n",
    "    \n",
    "        self.episode_score = 0\n",
    "        self.score_after_update = 0\n",
    "        self.score_before_update = 0\n",
    "        self.done = False\n",
    "    \n",
    "        self.camera = None\n",
    "        \n",
    "        self.torch_collected = False\n",
    "        self.time_taken = 0\n",
    "\n",
    "        # The shader toy and 'channels' we'll be using\n",
    "        self.shadertoy = None\n",
    "        self.channel0 = None\n",
    "        self.channel1 = None\n",
    "        self.load_shader()\n",
    "\n",
    "        # Sprites and sprite lists\n",
    "        self.player_sprite = None\n",
    "        self.torch = None #TF add \n",
    "        self.wall_list = arcade.SpriteList()\n",
    "        self.player_list = arcade.SpriteList()\n",
    "        self.bomb_list = arcade.SpriteList() #TF add\n",
    "        self.reward_list = arcade.SpriteList() #TF add\n",
    "        self.torch_list = arcade.SpriteList() #TF add\n",
    "        self.physics_engine = None\n",
    "        \n",
    "        self.gui_camera = None #TF added gui camera that can be used to draw gui elements\n",
    "        self.score = 0 #TF added score\n",
    "        self.scene = None #TF added scene\n",
    "\n",
    "        self.generate_sprites()\n",
    "        \n",
    "        #TF: Load sounds\n",
    "        self.collect_bomb_sound = arcade.load_sound(\":resources:sounds/explosion2.wav\")\n",
    "        self.collect_reward_sound = arcade.load_sound(\":resources:sounds/gameover2.wav\")\n",
    "        arcade.set_background_color(arcade.color.AMARANTH)\n",
    "        \n",
    "        #TF Light tutorial\n",
    "        self.light_layer = None\n",
    "        # Individual light we move with player, and turn on/off\n",
    "        self.player_light = None\n",
    "        LightEnv.center_window(self) #Display game window in center of pc screen\n",
    "\n",
    "\n",
    "    def load_shader(self):\n",
    "        # Where is the shader file? Must be specified as a path.\n",
    "        # shader_file_path = Path(\"added_light_source-Copy.glsl\")\n",
    "        # shader_file_path = Path(\"added_light_source.glsl\")\n",
    "\n",
    "        # Size of the window\n",
    "        window_size = self.get_size()\n",
    "\n",
    "        # Create the shader toy\n",
    "        # self.shadertoy = Shadertoy.create_from_file(window_size, shader_file_path)\n",
    "        self.shadertoy = Shadertoy.create_from_file(window_size, '/content/drive/My Drive/Colab Notebooks/INM363 Project/added_light_source.glsl')\n",
    "\n",
    "        # Create the channels 0 and 1 frame buffers.\n",
    "        # Make the buffer the size of the window, with 4 channels (RGBA)\n",
    "        self.channel0 = self.shadertoy.ctx.framebuffer(\n",
    "            color_attachments=[self.shadertoy.ctx.texture(window_size, components=4)]\n",
    "        )\n",
    "        self.channel1 = self.shadertoy.ctx.framebuffer(\n",
    "            color_attachments=[self.shadertoy.ctx.texture(window_size, components=4)]\n",
    "        )\n",
    "\n",
    "        # Assign the frame buffers to the channels\n",
    "        self.shadertoy.channel_0 = self.channel0.color_attachments[0]\n",
    "        self.shadertoy.channel_1 = self.channel1.color_attachments[0]\n",
    "                \n",
    "    def generate_sprites(self):\n",
    "        self.scene = arcade.Scene() # TF initialise scene\n",
    "        self.camera = arcade.Camera(SCREEN_WIDTH,SCREEN_HEIGHT)\n",
    "        self.gui_camera = arcade.Camera(self.width, self.height) #TF initialise GUI camera for displaying score\n",
    "        self.score = 0 #TF keep track of the score\n",
    "        \n",
    "        # -- Set up several columns of walls\n",
    "        for x in range(0, PLAYING_FIELD_WIDTH, 128):\n",
    "            for y in range(0, PLAYING_FIELD_HEIGHT, int(128 * SPRITE_SCALING)):\n",
    "                # Randomly skip a box so the player can find a way through\n",
    "                if random.randrange(2) > 0:\n",
    "                    wall = arcade.Sprite(\":resources:images/tiles/boxCrate_double.png\", SPRITE_SCALING)\n",
    "                    wall.center_x = x\n",
    "                    wall.center_y = y\n",
    "                    self.wall_list.append(wall)\n",
    "\n",
    "        # -- Set some hidden bombs in the area\n",
    "        for i in range(BOMB_COUNT):\n",
    "            bomb = arcade.Sprite(\":resources:images/tiles/bomb.png\", 0.25)\n",
    "            placed = False\n",
    "            while not placed:\n",
    "                bomb.center_x = random.randrange(PLAYING_FIELD_WIDTH)\n",
    "                bomb.center_y = random.randrange(PLAYING_FIELD_HEIGHT)\n",
    "                if not arcade.check_for_collision_with_list(bomb, self.wall_list):\n",
    "                    placed = True\n",
    "            self.bomb_list.append(bomb)\n",
    "            self.scene.add_sprite(\"Bombs\", bomb) # TF add bombs to scene\n",
    "\n",
    "            \n",
    "        #TF Start - adding in reward sprite\n",
    "        for i in range(REWARD_COUNT):\n",
    "            reward = arcade.Sprite(\":resources:images/tiles/signExit.png\", 0.25)\n",
    "            placed = False\n",
    "            while not placed:\n",
    "                reward.center_x = random.randrange(PLAYING_FIELD_WIDTH)\n",
    "                reward.center_y = random.randrange(PLAYING_FIELD_HEIGHT)\n",
    "                if not arcade.check_for_collision_with_list(reward, self.wall_list):\n",
    "                    placed = True\n",
    "            self.reward_list.append(reward)\n",
    "            self.scene.add_sprite(\"Reward\", reward) # add reward to scene\n",
    "        #TF End - adding in reward sprite\n",
    "        \n",
    "        # Create the player\n",
    "        self.player_sprite = arcade.Sprite(\":resources:images/animated_characters/female_person/femalePerson_idle.png\",\n",
    "                                           scale=SPRITE_SCALING)\n",
    "        self.player_sprite.center_x = 256\n",
    "        self.player_sprite.center_y = 512\n",
    "        self.player_list.append(self.player_sprite)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Create the torch - TF add\n",
    "        self.torch = arcade.Sprite(\":resources:images/tiles/torch1.png\",\n",
    "                                           scale=SPRITE_SCALING)\n",
    "        placed = False\n",
    "        while not placed:\n",
    "            self.torch.center_x = random.randrange(PLAYING_FIELD_WIDTH)\n",
    "            self.torch.center_y = random.randrange(PLAYING_FIELD_HEIGHT)\n",
    "            if not arcade.check_for_collision_with_list(self.torch, self.wall_list):\n",
    "                placed = True             \n",
    "        self.torch_list.append(self.torch)\n",
    "        self.scene.add_sprite(\"Torch\", self.torch) # TF add torch to scene\n",
    "\n",
    "        # Physics engine, so we don't run into walls\n",
    "        self.physics_engine = arcade.PhysicsEngineSimple(self.player_sprite, self.wall_list)\n",
    "\n",
    "    def on_draw(self):\n",
    "\n",
    "        # Select the channel 0 frame buffer to draw on\n",
    "        self.channel0.use()\n",
    "        self.channel0.clear()\n",
    "        # Draw the walls\n",
    "        self.wall_list.draw()\n",
    "        \n",
    "        self.channel1.use()\n",
    "        self.channel1.clear()\n",
    "        # Draw the bombs\n",
    "        self.bomb_list.draw()\n",
    "        \n",
    "        # TF Start - Draw the reward\n",
    "        self.reward_list.draw()\n",
    "\n",
    "        # Draw the walls BEFORE light has been calculated - more realistic.\n",
    "        self.wall_list.draw()\n",
    "        \n",
    "        # Select this window to draw on\n",
    "        self.use()\n",
    "        # Clear to background color\n",
    "        self.clear()\n",
    "\n",
    "        # Run the shader and render to the window\n",
    "        if self.torch_collected == False:\n",
    "            self.shadertoy.program['lightPosition'] = self.torch.position #TF add\n",
    "            self.shadertoy.program['lightSize'] = 50\n",
    "#             self.shadertoy.program['lightPosition'] = self.player_sprite.position\n",
    "#             self.shadertoy.program['lightSize'] = 150\n",
    "            self.shadertoy.render()\n",
    "        elif self.torch_collected == True:\n",
    "            self.shadertoy.program['lightPosition'] = self.player_sprite.position #TF add\n",
    "            self.shadertoy.program['lightSize'] = 500\n",
    "#             self.shadertoy.program['lightPosition2'] = self.player_sprite.position\n",
    "#             self.shadertoy.program['lightSize2'] = 500\n",
    "            self.shadertoy.render()            \n",
    "\n",
    "        # Draw the walls after light has been calculated\n",
    "#         self.wall_list.draw()\n",
    "\n",
    "        self.camera.use()\n",
    "\n",
    "        #TF Start - Adding camera to display score AFTER light has been calculated\n",
    "        #Activate the GUI camera before drawing GUI elements\n",
    "        self.gui_camera.use()\n",
    "\n",
    "        #Draw our score on the screen, scrolling it with the viewport\n",
    "        score_text = f\"score: {self.score}, time taken: {round(self.time_taken)}\"\n",
    "        arcade.draw_text(\n",
    "            score_text,\n",
    "            10,\n",
    "            10,\n",
    "            arcade.csscolor.WHITE,\n",
    "            18,\n",
    "        )\n",
    "        #TF End - Adding camera to display score AFTER light has been calculated\n",
    "\n",
    "        # Draw the player\n",
    "        self.player_list.draw()\n",
    "        self.torch_list.draw()\n",
    "        \n",
    "        image = arcade.get_image()\n",
    "#         image.save(\"test.png\")\n",
    "        \n",
    "    def on_key_press(self, key, modifiers):\n",
    "        \"\"\"Called whenever a key is pressed. \"\"\"\n",
    "\n",
    "        if key == arcade.key.UP:\n",
    "            self.player_sprite.change_y = PLAYER_MOVEMENT_SPEED\n",
    "        elif key == arcade.key.DOWN:\n",
    "            self.player_sprite.change_y = -PLAYER_MOVEMENT_SPEED\n",
    "        elif key == arcade.key.LEFT:\n",
    "            self.player_sprite.change_x = -PLAYER_MOVEMENT_SPEED\n",
    "        elif key == arcade.key.RIGHT:\n",
    "            self.player_sprite.change_x = PLAYER_MOVEMENT_SPEED\n",
    "\n",
    "    def on_key_release(self, key, modifiers):\n",
    "        \"\"\"Called when the user releases a key. \"\"\"\n",
    "\n",
    "        if key == arcade.key.UP or key == arcade.key.DOWN:\n",
    "            self.player_sprite.change_y = 0\n",
    "        elif key == arcade.key.LEFT or key == arcade.key.RIGHT:\n",
    "            self.player_sprite.change_x = 0\n",
    "\n",
    "    def on_update(self, delta_time: float):\n",
    "        \"\"\" Movement and game logic \"\"\"\n",
    "        self.score_before_update = self.score\n",
    "        \n",
    "        self.time_before_update = self.time_taken_reported() #test\n",
    "        \n",
    "        self.time_taken += delta_time\n",
    "\n",
    "        self.time_after_update = self.time_taken_reported() #test\n",
    "        \n",
    "        if self.time_after_update - self.time_before_update == 1:\n",
    "#             print(\"-1 time penalty to score\")\n",
    "            self.score -= 1 \n",
    "        # Call update on all sprites\n",
    "        \n",
    "        # Keep the player on screen -TF add from https://realpython.com/arcade-python-game-framework/#drawing-on-the-window\n",
    "        if self.player_sprite.top > self.height:\n",
    "            self.player_sprite.top = self.height\n",
    "        if self.player_sprite.right > self.width:\n",
    "            self.player_sprite.right = self.width\n",
    "        if self.player_sprite.bottom < 0:\n",
    "            self.player_sprite.bottom = 0\n",
    "        if self.player_sprite.left < 0:\n",
    "            self.player_sprite.left = 0\n",
    "        \n",
    "        self.physics_engine.update()\n",
    "        \n",
    "        #TF - Start Testing:\n",
    "        #See if we hit any bombs\n",
    "        bomb_hit_list = arcade.check_for_collision_with_list(\n",
    "            self.player_sprite, self.scene[\"Bombs\"]\n",
    "        )\n",
    "        #See if we hit any rewards\n",
    "        reward_hit_list = arcade.check_for_collision_with_list(\n",
    "            self.player_sprite, self.scene[\"Reward\"]\n",
    "        )\n",
    "        #See if we hit any torches\n",
    "        torch_hit_list = arcade.check_for_collision_with_list(\n",
    "            self.player_sprite, self.scene[\"Torch\"]\n",
    "        )\n",
    "        \n",
    "        #Loop through each bomb we hit (if any) and remove it\n",
    "        for bomb in bomb_hit_list:\n",
    "            #Remove the bomb\n",
    "            bomb.remove_from_sprite_lists()\n",
    "            #Play a sound\n",
    "            arcade.play_sound(self.collect_bomb_sound)\n",
    "            #Minus 1 to score\n",
    "            self.score -= 1\n",
    "            \n",
    "#             #Showing image for wrapper testing\n",
    "#             x = arcade.get_image()\n",
    "#             x.show()\n",
    "\n",
    "        #If reward is hit, then remove it and +100 score.\n",
    "        for reward in reward_hit_list:\n",
    "            #Remove the reward\n",
    "            reward.remove_from_sprite_lists()\n",
    "            #Play a sound\n",
    "            arcade.play_sound(self.collect_reward_sound)\n",
    "            #Plus 100 to score\n",
    "            self.score += 100\n",
    "            END_GAME = True #TF - Flag for ending game when reward collected\n",
    "            print(f\"Game completed with a score of: {self.score} at time: {round(self.time_taken)}\")\n",
    "            self.done = True\n",
    "            \n",
    "        #If torch is hit, then remove it.\n",
    "        for torch in torch_hit_list:\n",
    "            #Remove the torch\n",
    "            self.torch_collected = True\n",
    "            torch.remove_from_sprite_lists()\n",
    "            #Play a sound\n",
    "            arcade.play_sound(self.collect_reward_sound)\n",
    "            self.physics_engine.update()\n",
    "            \n",
    "\n",
    "\n",
    "                        \n",
    "        #TF attempt to reposition player back to center after bomb collision\n",
    "        if bomb_hit_list != []:     \n",
    "            self.player_sprite.center_x = 256 #Might need to paramaterise this to be screen size\n",
    "            self.player_sprite.center_y = 512 #As resizing window to 80x60 could ruin this\n",
    "            #Should be ok not to parameterise actually as game calulcated as normal then passed and\n",
    "            #converted to 80x60 in the wrapper, so it's all scaled.\n",
    "#         TF - End Testing\n",
    "     \n",
    "        self.score_after_update = self.score\n",
    "#         print(\"self.score after update: \", self.score_after_update)\n",
    "\n",
    "        self.frame += 1\n",
    "        \n",
    "    def reset(self):\n",
    "        print(\"resetting\")\n",
    "        \"\"\"\n",
    "        This function resets the environment to its original state (time = 0).\n",
    "        Then it places the agent at start position and exit at a new random location.\n",
    "        \n",
    "        It is common practice to return the observations, \n",
    "        so that the agent can decide on the first action right after the resetting of the environment.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "#         arcade.close_window()\n",
    "#         arcade.set_window()\n",
    "#         arcade.finish_render()\n",
    "\n",
    "        self.torch_collected = False\n",
    "        self.time_taken = 0\n",
    "\n",
    "        # The shader toy and 'channels' we'll be using\n",
    "        self.shadertoy = None\n",
    "        self.channel0 = None\n",
    "        self.channel1 = None\n",
    "        self.load_shader()\n",
    "\n",
    "        # Sprites and sprite lists\n",
    "        self.player_sprite = None\n",
    "        self.torch = None #TF add \n",
    "        self.wall_list = arcade.SpriteList()\n",
    "        self.player_list = arcade.SpriteList()\n",
    "        self.bomb_list = arcade.SpriteList() #TF add\n",
    "        self.reward_list = arcade.SpriteList() #TF add\n",
    "        self.torch_list = arcade.SpriteList() #TF add\n",
    "        self.physics_engine = None\n",
    "        \n",
    "        self.gui_camera = None #TF added gui camera that can be used to draw gui elements\n",
    "        self.score = 0 #TF added score\n",
    "        self.episode_score = 0 #Reset episode score to 0 at beginning of each episode\n",
    "        self.scene = None #TF added scene\n",
    "\n",
    "        self.generate_sprites()\n",
    "        \n",
    "        #Get the image object of the current window\n",
    "        obs = arcade.get_image()\n",
    "        \n",
    "        self.done = False #set done flag to false ready for next episode\n",
    "    \n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        #Reset episode score to 0 at beginning of step action\n",
    "        self.step_score = 0\n",
    "        \n",
    "        #Move agent in direction chosen by rllib. (input was 0,1,2,3, changed to up,down,left,right).\n",
    "        if action == 'up':\n",
    "            self.player_sprite.change_y = PLAYER_MOVEMENT_SPEED\n",
    "        elif action == 'down':\n",
    "            self.player_sprite.change_y = -PLAYER_MOVEMENT_SPEED\n",
    "        elif action == 'left':\n",
    "            self.player_sprite.change_x = -PLAYER_MOVEMENT_SPEED\n",
    "        elif action == 'right':\n",
    "            self.player_sprite.change_x = PLAYER_MOVEMENT_SPEED\n",
    "        \n",
    "        # Calculate the reward for the particular action (0, -1 or +100).\n",
    "        self.step_score = self.score_after_update - self.score_before_update\n",
    "#         print(f\"score_after_update: {self.score_after_update}, score_before_update: {self.score_before_update}\")\n",
    "        reward = self.step_score #this needs to be old episode score - new episode score\n",
    "#         print(\"step in game reward: \", reward)\n",
    "        obs = arcade.get_image()\n",
    "        \n",
    "        return obs, reward, self.done\n",
    "    \n",
    "    def stop_movement(self, action):\n",
    "        print(\"stopping movement\")\n",
    "        \"\"\"Called when the player makes a move. \"\"\"\n",
    "\n",
    "        if action == 'up' or action == 'down':\n",
    "            self.player_sprite.change_y = 0\n",
    "        elif action == 'left' or action == 'right':\n",
    "            self.player_sprite.change_x = 0\n",
    "    \n",
    "    def time_taken_reported(self):\n",
    "        x = round(self.time_taken)\n",
    "        return x\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eG4VEI3vvt6e",
   "metadata": {
    "id": "eG4VEI3vvt6e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MMHAz_kuZ-Tr",
   "metadata": {
    "id": "MMHAz_kuZ-Tr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f710c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5f710c7",
    "outputId": "ece6d623-8033-4bee-b6c9-a38913bd6342"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:ray.rllib.utils.compression:lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random, math\n",
    "import numpy as np\n",
    "import arcade\n",
    "from skimage import data, color\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from PIL import Image\n",
    "\n",
    "        \n",
    "# from LightEnvCopy import LightEnv\n",
    "# from LightEnvCopy import LightEnv\n",
    "\n",
    "import gym.spaces\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "# Do the math to figure out our screen dimensions\n",
    "SCREEN_WIDTH = 800\n",
    "SCREEN_HEIGHT = 600\n",
    "SCREEN_TITLE = \"Game 1: Let There Be Light!\"\n",
    "\n",
    "SPRITE_SCALING = 0.25\n",
    "\n",
    "# How fast the camera pans to the player. 1.0 is instant.\n",
    "CAMERA_SPEED = 0.1\n",
    "\n",
    "PLAYER_MOVEMENT_SPEED = 7\n",
    "BOMB_COUNT = 5\n",
    "TORCH_COUNT = 1\n",
    "PLAYING_FIELD_WIDTH = 800 #1600\n",
    "PLAYING_FIELD_HEIGHT = 600 #1600\n",
    "REWARD_COUNT = 1 #TF - Add in reward\n",
    "END_GAME = False\n",
    "torch_collected = False\n",
    "\n",
    "#TF Start - Adding in actions for action conversion\n",
    "\n",
    "# COnvenient data structure to hold information about actions\n",
    "Action = namedtuple('Action', 'name index delta_i delta_j')\n",
    "\n",
    "up = Action('up', 0, -1, 0)    \n",
    "down = Action('down', 1, 1, 0)    \n",
    "left = Action('left', 2, 0, -1)    \n",
    "right = Action('right', 3, 0, 1)    \n",
    "\n",
    "index_to_actions = {}\n",
    "for action in [up, down, left, right]:\n",
    "    index_to_actions[action.index] = action\n",
    "# print(index_to_actions[0].name)\n",
    "str_to_actions = {}\n",
    "for action in [up, down, left, right]:\n",
    "    str_to_actions[action.name] = action\n",
    "#TF End - Adding in actions for action conversion\n",
    "\n",
    "\n",
    "class LightEnvWrapper(gym.Env, LightEnv):\n",
    "    \"\"\"Class that wraps the Lights Environment to make it \n",
    "    compatible with RLLib.\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"rgb_array\", \"state_pixels\"]}\n",
    "    \n",
    "    def __init__(self, config: EnvContext):\n",
    "        super().__init__(SCREEN_WIDTH, SCREEN_HEIGHT, SCREEN_TITLE)\n",
    "        self.counting = 0\n",
    "        game_size = config.get(\"size_env\")\n",
    "\n",
    "#         self.mygame = GUIWrapper(LightEnv)\n",
    "        \n",
    "        self.mygame = LightEnv\n",
    "        #The action space is a choice of 9 actions: U/D/L/R/UR/DR/DL/UL/DO NOTHING. Not continuous\n",
    "        #because speed of agent is fixed. Or potentially just 4: U/D/L/R.\n",
    "        self.action_space = Discrete(4)\n",
    "        #The observation space is a fixed image of the current game screen - fully observable.\n",
    "        #Can set to a view just around the player using arcade.set_viewport.\n",
    "        #Need obs space to be either 42,42,x or 84,84,x to be compatible with rllib.\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84,84, 4), dtype=np.uint8)\n",
    "        \n",
    "        self.counting = 0\n",
    "\n",
    "    def reset(self):\n",
    "        print(\"resetting in wrapper\")\n",
    "        self.render()\n",
    "        #Resets the state of the environment for a new episode and an initial observation.\n",
    "        obs_mygame = self.mygame.reset(self)\n",
    "        \n",
    "        #Open up the resetted image to verify working correctly.\n",
    "        obs_mygame.show()\n",
    "        \n",
    "        #Convert observation to 84x84 resolution and np array for rllib.\n",
    "        obs = self.convert_observations(obs_mygame)\n",
    "        \n",
    "#         print(\"resetted\")\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        self.counting += 1\n",
    "#         print(\"Wrapper stepping number: \", self.counting) #every 33 is a second in game time.\n",
    "#         print(\"stepping\")\n",
    "        #Choose a random action: 0, 1, 2, 3.\n",
    "        assert action in [0, 1, 2, 3] #0-up,1-down,2-left,3-right.\n",
    "        \n",
    "        #Convert the numeric action to a keyword: up, down, left, right.\n",
    "        actions_myenv = index_to_actions[action].name #returns a word, one of: up/down/left/right\n",
    "        # print(f\"action taken: {actions_myenv}\")\n",
    "        \n",
    "        #Update the window with on_update()\n",
    "        self.render()\n",
    "        \n",
    "        #Compute observation extracted from the window (800x600), with reward and done flag.\n",
    "        obs, reward, done = self.mygame.step(self,actions_myenv)\n",
    "        \n",
    "        # if self.counting % 33 == 0:\n",
    "#             print(\"self.counting is now divisible by 33(ie. 1 second has passed), showing obs now\")\n",
    "            # print(f\"total score is {self.score} at time: {self.mygame.time_taken_reported(self)}\")\n",
    "            # obs.show()\n",
    "            \n",
    "#         print(f\"step reward is {reward}\")\n",
    "        \n",
    "        #Convert observation to 84x84 resolution and np array for rllib.\n",
    "        obs_mygame = self.convert_observations(obs)\n",
    "        \n",
    "        #If the reward has been obtained, reset the environment and start again\n",
    "        if done == True:\n",
    "            print(f\"done is {done}, resetting environment in wrapper.\")\n",
    "            obs.save(\"test.png\") \n",
    "            self.reset()\n",
    "        \n",
    "#         print(\"Finished stepping\")\n",
    "        return obs_mygame, reward, done, {}\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        random.seed(seed)\n",
    "\n",
    "    def convert_observations(self, obs_mygame): #Not needed? This is just for rescaling?\n",
    "        # We normalize and concatenate observations\n",
    "        obs = obs_mygame\n",
    "        obs_resized = obs.resize((84,84))\n",
    "        obsarray = np.array(obs_resized)\n",
    "#         print(\"sum: \", np.sum(obsarray))\n",
    "        return obsarray\n",
    "    \n",
    "    def render(self):\n",
    "        self.mygame.on_update(self, 1/60)\n",
    "        self.mygame.on_draw(self)\n",
    "        test = self.mygame.time_taken_reported(self)\n",
    "#         print(\"testing time taken: \", test)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84520340",
   "metadata": {
    "id": "84520340"
   },
   "source": [
    "### Now run the rllib script to train the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psPZ7vYCM41p",
   "metadata": {
    "id": "psPZ7vYCM41p"
   },
   "source": [
    "DQN Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e825c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96e825c5",
    "outputId": "2039f92f-6086-4adf-aeff-564f1dd4cec9",
    "scrolled": false
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 14:05:24,855\tWARNING deprecation.py:48 -- DeprecationWarning: `ray.rllib.agents.dqn` has been deprecated. Use `ray.rllib.algorithms.[dqn|simple_q|r2d2|apex_dqn]` instead. This will raise an error in the future!\n",
      "WARNING:ray.tune.logger.unified:Could not instantiate TBXLogger: No module named 'tensorboardX'.\n",
      "2022-09-15 14:05:24,882\tWARNING deprecation.py:48 -- DeprecationWarning: `ray.rllib.algorithms.dqn.dqn.DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.algorithms.dqn.dqn.DQNConfig(...)` instead. This will raise an error in the future!\n",
      "2022-09-15 14:05:24,884\tWARNING deprecation.py:48 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n",
      "/usr/local/lib/python3.7/dist-packages/ray/_private/ray_option_utils.py:273: DeprecationWarning: Setting 'object_store_memory' for actors is deprecated since it doesn't actually reserve the required object store memory. Use object spilling that's enabled by default (https://docs.ray.io/en/releases-2.0.0/ray-core/objects/object-spilling.html) instead to bypass the object store memory size limitation.\n",
      "  stacklevel=1,\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 1, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.LightEnvWrapper'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 0, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 4, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'rllib', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0005, 'train_batch_size': 32, 'model': {'fcnet_hiddens': [64, 64], 'fcnet_activation': 'relu'}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.02, 'epsilon_timesteps': 10000}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 1, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 1000, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'target_network_update_freq': 500, 'replay_buffer_config': {'type': 'MultiAgentPrioritizedReplayBuffer', 'prioritized_replay': -1, 'capacity': 50000, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'replay_sequence_length': 1, 'worker_side_prioritization': False}, 'store_buffer_in_checkpoints': False, 'lr_schedule': None, 'adam_epsilon': 1e-08, 'grad_clip': 40, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'hiddens': [256], 'double_q': False, 'n_step': 1, 'before_learn_on_batch': None, 'training_intensity': None, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 14:05:27,111\tINFO worker.py:1518 -- Started a local Ray instance.\n",
      "2022-09-15 14:05:29,322\tWARNING env.py:143 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting in wrapper\n",
      "resetting\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 14:05:34,626\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting in wrapper\n",
      "resetting\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 14:05:35,271\tWARNING deprecation.py:48 -- DeprecationWarning: `ReplayBuffer.add_batch()` has been deprecated. Use `ReplayBuffer.add()` instead. This will raise an error in the future!\n",
      "2022-09-15 14:05:35,310\tWARNING replay_buffer.py:61 -- Estimated max memory usage for replay buffer is 2.82545 GB (50000.0 batches of size 1, 56509 bytes each), available system memory is 13.616361472 GB\n",
      "2022-09-15 14:05:35,330\tWARNING multi_agent_prioritized_replay_buffer.py:221 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "Exception ignored in: <function OpenALDriver.__del__ at 0x7f39927ab0e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyglet/media/drivers/openal/adaptation.py\", line 63, in __del__\n",
      "    self.delete()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyglet/media/drivers/openal/adaptation.py\", line 70, in delete\n",
      "    self.worker.stop()\n",
      "AttributeError: 'OpenALDriver' object has no attribute 'worker'\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game completed with a score of: 98 at time: 2\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 14:05:58,075\tWARNING deprecation.py:48 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode reward mean:  98.0\n",
      "checkpoint saved at /root/ray_results/DQN_LightEnvWrapper_2022-09-15_14-05-24733puria/checkpoint_000001\n",
      "episode reward mean:  98.0\n",
      "episode reward mean:  98.0\n",
      "episode reward mean:  98.0\n",
      "episode reward mean:  98.0\n",
      "episode reward mean:  98.0\n",
      "episode reward mean:  98.0\n",
      "episode reward mean:  98.0\n",
      "episode reward mean:  98.0\n",
      "episode reward mean:  98.0\n",
      "episode reward mean:  98.0\n",
      "episode reward mean:  98.0\n",
      "episode reward mean:  98.0\n",
      "episode reward mean:  98.0\n",
      "episode reward mean:  98.0\n",
      "episode reward mean:  98.0\n",
      "episode reward mean:  98.0\n",
      "episode reward mean:  98.0\n",
      "Game completed with a score of: -214 at time: 309\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -58.0\n",
      "episode reward mean:  -58.0\n",
      "episode reward mean:  -58.0\n",
      "episode reward mean:  -58.0\n",
      "Game completed with a score of: 39 at time: 56\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "Game completed with a score of: 97 at time: 2\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  5.0\n",
      "Game completed with a score of: 82 at time: 17\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "Game completed with a score of: 100 at time: 0\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  33.666666666666664\n",
      "episode reward mean:  33.666666666666664\n",
      "Game completed with a score of: 56 at time: 40\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  36.857142857142854\n",
      "episode reward mean:  36.857142857142854\n",
      "Game completed with a score of: 63 at time: 32\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  40.125\n",
      "episode reward mean:  40.125\n",
      "episode reward mean:  40.125\n",
      "Game completed with a score of: 43 at time: 54\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  40.44444444444444\n",
      "episode reward mean:  40.44444444444444\n",
      "episode reward mean:  40.44444444444444\n",
      "Game completed with a score of: 52 at time: 45\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "Game completed with a score of: 90 at time: 8\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  46.0\n",
      "episode reward mean:  46.0\n",
      "episode reward mean:  46.0\n",
      "episode reward mean:  46.0\n",
      "episode reward mean:  46.0\n",
      "episode reward mean:  46.0\n",
      "episode reward mean:  46.0\n",
      "episode reward mean:  46.0\n",
      "episode reward mean:  46.0\n",
      "episode reward mean:  46.0\n",
      "Game completed with a score of: -64 at time: 161\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  36.833333333333336\n",
      "Game completed with a score of: 87 at time: 12\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  40.69230769230769\n",
      "episode reward mean:  40.69230769230769\n",
      "episode reward mean:  40.69230769230769\n",
      "Game completed with a score of: 45 at time: 51\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  41.0\n",
      "episode reward mean:  41.0\n",
      "Game completed with a score of: 58 at time: 40\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  42.13333333333333\n",
      "episode reward mean:  42.13333333333333\n",
      "Game completed with a score of: 73 at time: 25\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "Game completed with a score of: 100 at time: 0\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  47.35294117647059\n",
      "Game completed with a score of: 70 at time: 28\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  48.611111111111114\n",
      "Game completed with a score of: 93 at time: 7\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  50.94736842105263\n",
      "Game completed with a score of: 75 at time: 24\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  52.15\n",
      "episode reward mean:  52.15\n",
      "Game completed with a score of: 62 at time: 35\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  52.61904761904762\n",
      "episode reward mean:  52.61904761904762\n",
      "episode reward mean:  52.61904761904762\n",
      "Game completed with a score of: 58 at time: 41\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  52.86363636363637\n",
      "Game completed with a score of: 83 at time: 15\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  54.17391304347826\n",
      "Game completed with a score of: 80 at time: 19\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  55.25\n",
      "episode reward mean:  55.25\n",
      "episode reward mean:  55.25\n",
      "episode reward mean:  55.25\n",
      "episode reward mean:  55.25\n",
      "episode reward mean:  55.25\n",
      "episode reward mean:  55.25\n",
      "episode reward mean:  55.25\n",
      "episode reward mean:  55.25\n",
      "episode reward mean:  55.25\n",
      "episode reward mean:  55.25\n",
      "episode reward mean:  55.25\n",
      "episode reward mean:  55.25\n",
      "episode reward mean:  55.25\n",
      "episode reward mean:  55.25\n",
      "episode reward mean:  55.25\n",
      "episode reward mean:  55.25\n",
      "Game completed with a score of: -197 at time: 292\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  45.16\n",
      "episode reward mean:  45.16\n",
      "episode reward mean:  45.16\n",
      "Game completed with a score of: 57 at time: 40\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  45.61538461538461\n",
      "episode reward mean:  45.61538461538461\n",
      "Game completed with a score of: 66 at time: 30\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "Game completed with a score of: 88 at time: 11\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  47.857142857142854\n",
      "episode reward mean:  47.857142857142854\n",
      "episode reward mean:  47.857142857142854\n",
      "episode reward mean:  47.857142857142854\n",
      "episode reward mean:  47.857142857142854\n",
      "episode reward mean:  47.857142857142854\n",
      "episode reward mean:  47.857142857142854\n",
      "episode reward mean:  47.857142857142854\n",
      "Game completed with a score of: -29 at time: 125\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  45.206896551724135\n",
      "episode reward mean:  45.206896551724135\n",
      "Game completed with a score of: 66 at time: 30\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  45.9\n",
      "episode reward mean:  45.9\n",
      "Game completed with a score of: 59 at time: 37\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "Game completed with a score of: 97 at time: 2\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  47.90625\n",
      "Game completed with a score of: 81 at time: 16\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  48.90909090909091\n",
      "episode reward mean:  48.90909090909091\n",
      "episode reward mean:  48.90909090909091\n",
      "episode reward mean:  48.90909090909091\n",
      "episode reward mean:  48.90909090909091\n",
      "checkpoint saved at /root/ray_results/DQN_LightEnvWrapper_2022-09-15_14-05-24733puria/checkpoint_000101\n",
      "episode reward mean:  48.90909090909091\n",
      "Game completed with a score of: -1 at time: 98\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  47.44117647058823\n",
      "episode reward mean:  47.44117647058823\n",
      "episode reward mean:  47.44117647058823\n",
      "Game completed with a score of: 44 at time: 52\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  47.34285714285714\n",
      "episode reward mean:  47.34285714285714\n",
      "episode reward mean:  47.34285714285714\n",
      "Game completed with a score of: 46 at time: 52\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  47.30555555555556\n",
      "episode reward mean:  47.30555555555556\n",
      "episode reward mean:  47.30555555555556\n",
      "episode reward mean:  47.30555555555556\n",
      "episode reward mean:  47.30555555555556\n",
      "Game completed with a score of: 22 at time: 74\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  46.62162162162162\n",
      "episode reward mean:  46.62162162162162\n",
      "episode reward mean:  46.62162162162162\n",
      "episode reward mean:  46.62162162162162\n",
      "episode reward mean:  46.62162162162162\n",
      "episode reward mean:  46.62162162162162\n",
      "episode reward mean:  46.62162162162162\n",
      "Game completed with a score of: -29 at time: 124\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  44.63157894736842\n",
      "episode reward mean:  44.63157894736842\n",
      "episode reward mean:  44.63157894736842\n",
      "episode reward mean:  44.63157894736842\n",
      "episode reward mean:  44.63157894736842\n",
      "episode reward mean:  44.63157894736842\n",
      "episode reward mean:  44.63157894736842\n",
      "episode reward mean:  44.63157894736842\n",
      "episode reward mean:  44.63157894736842\n",
      "episode reward mean:  44.63157894736842\n",
      "episode reward mean:  44.63157894736842\n",
      "episode reward mean:  44.63157894736842\n",
      "episode reward mean:  44.63157894736842\n",
      "episode reward mean:  44.63157894736842\n",
      "episode reward mean:  44.63157894736842\n",
      "episode reward mean:  44.63157894736842\n",
      "episode reward mean:  44.63157894736842\n",
      "episode reward mean:  44.63157894736842\n",
      "episode reward mean:  44.63157894736842\n",
      "episode reward mean:  44.63157894736842\n",
      "Game completed with a score of: -239 at time: 334\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "episode reward mean:  37.35897435897436\n",
      "Game completed with a score of: -379 at time: 474\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "episode reward mean:  26.95\n",
      "Game completed with a score of: -410 at time: 508\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  16.29268292682927\n",
      "episode reward mean:  16.29268292682927\n",
      "checkpoint saved at /root/ray_results/DQN_LightEnvWrapper_2022-09-15_14-05-24733puria/checkpoint_000201\n",
      "Game completed with a score of: 56 at time: 43\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  17.238095238095237\n",
      "episode reward mean:  17.238095238095237\n",
      "episode reward mean:  17.238095238095237\n",
      "episode reward mean:  17.238095238095237\n",
      "episode reward mean:  17.238095238095237\n",
      "episode reward mean:  17.238095238095237\n",
      "episode reward mean:  17.238095238095237\n",
      "episode reward mean:  17.238095238095237\n",
      "episode reward mean:  17.238095238095237\n",
      "episode reward mean:  17.238095238095237\n",
      "episode reward mean:  17.238095238095237\n",
      "episode reward mean:  17.238095238095237\n",
      "episode reward mean:  17.238095238095237\n",
      "episode reward mean:  17.238095238095237\n",
      "episode reward mean:  17.238095238095237\n",
      "episode reward mean:  17.238095238095237\n",
      "episode reward mean:  17.238095238095237\n",
      "Game completed with a score of: -185 at time: 283\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "episode reward mean:  12.534883720930232\n",
      "Game completed with a score of: -838 at time: 933\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -6.795454545454546\n",
      "episode reward mean:  -6.795454545454546\n",
      "episode reward mean:  -6.795454545454546\n",
      "episode reward mean:  -6.795454545454546\n",
      "episode reward mean:  -6.795454545454546\n",
      "episode reward mean:  -6.795454545454546\n",
      "episode reward mean:  -6.795454545454546\n",
      "Game completed with a score of: -8 at time: 108\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "checkpoint saved at /root/ray_results/DQN_LightEnvWrapper_2022-09-15_14-05-24733puria/checkpoint_000301\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "episode reward mean:  -6.822222222222222\n",
      "Game completed with a score of: -411 at time: 506\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "Game completed with a score of: 99 at time: 0\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -13.170212765957446\n",
      "episode reward mean:  -13.170212765957446\n",
      "episode reward mean:  -13.170212765957446\n",
      "episode reward mean:  -13.170212765957446\n",
      "episode reward mean:  -13.170212765957446\n",
      "episode reward mean:  -13.170212765957446\n",
      "episode reward mean:  -13.170212765957446\n",
      "episode reward mean:  -13.170212765957446\n",
      "episode reward mean:  -13.170212765957446\n",
      "episode reward mean:  -13.170212765957446\n",
      "episode reward mean:  -13.170212765957446\n",
      "episode reward mean:  -13.170212765957446\n",
      "episode reward mean:  -13.170212765957446\n",
      "episode reward mean:  -13.170212765957446\n",
      "episode reward mean:  -13.170212765957446\n",
      "episode reward mean:  -13.170212765957446\n",
      "Game completed with a score of: -175 at time: 271\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -16.541666666666668\n",
      "Game completed with a score of: 100 at time: 0\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -14.16326530612245\n",
      "episode reward mean:  -14.16326530612245\n",
      "episode reward mean:  -14.16326530612245\n",
      "episode reward mean:  -14.16326530612245\n",
      "episode reward mean:  -14.16326530612245\n",
      "episode reward mean:  -14.16326530612245\n",
      "episode reward mean:  -14.16326530612245\n",
      "episode reward mean:  -14.16326530612245\n",
      "episode reward mean:  -14.16326530612245\n",
      "episode reward mean:  -14.16326530612245\n",
      "episode reward mean:  -14.16326530612245\n",
      "episode reward mean:  -14.16326530612245\n",
      "episode reward mean:  -14.16326530612245\n",
      "episode reward mean:  -14.16326530612245\n",
      "episode reward mean:  -14.16326530612245\n",
      "episode reward mean:  -14.16326530612245\n",
      "episode reward mean:  -14.16326530612245\n",
      "episode reward mean:  -14.16326530612245\n",
      "episode reward mean:  -14.16326530612245\n",
      "Game completed with a score of: -231 at time: 327\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "checkpoint saved at /root/ray_results/DQN_LightEnvWrapper_2022-09-15_14-05-24733puria/checkpoint_000401\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "episode reward mean:  -18.5\n",
      "Game completed with a score of: -1980 at time: 2076\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "checkpoint saved at /root/ray_results/DQN_LightEnvWrapper_2022-09-15_14-05-24733puria/checkpoint_000501\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "episode reward mean:  -56.96078431372549\n",
      "Game completed with a score of: -498 at time: 596\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -65.4423076923077\n",
      "episode reward mean:  -65.4423076923077\n",
      "episode reward mean:  -65.4423076923077\n",
      "episode reward mean:  -65.4423076923077\n",
      "episode reward mean:  -65.4423076923077\n",
      "episode reward mean:  -65.4423076923077\n",
      "episode reward mean:  -65.4423076923077\n",
      "Game completed with a score of: -10 at time: 107\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -64.39622641509433\n",
      "episode reward mean:  -64.39622641509433\n",
      "episode reward mean:  -64.39622641509433\n",
      "episode reward mean:  -64.39622641509433\n",
      "episode reward mean:  -64.39622641509433\n",
      "episode reward mean:  -64.39622641509433\n",
      "episode reward mean:  -64.39622641509433\n",
      "episode reward mean:  -64.39622641509433\n",
      "episode reward mean:  -64.39622641509433\n",
      "episode reward mean:  -64.39622641509433\n",
      "episode reward mean:  -64.39622641509433\n",
      "Game completed with a score of: -89 at time: 188\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -64.85185185185185\n",
      "episode reward mean:  -64.85185185185185\n",
      "episode reward mean:  -64.85185185185185\n",
      "episode reward mean:  -64.85185185185185\n",
      "Game completed with a score of: 32 at time: 68\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -63.09090909090909\n",
      "episode reward mean:  -63.09090909090909\n",
      "episode reward mean:  -63.09090909090909\n",
      "episode reward mean:  -63.09090909090909\n",
      "episode reward mean:  -63.09090909090909\n",
      "episode reward mean:  -63.09090909090909\n",
      "episode reward mean:  -63.09090909090909\n",
      "episode reward mean:  -63.09090909090909\n",
      "Game completed with a score of: -27 at time: 123\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -62.44642857142857\n",
      "episode reward mean:  -62.44642857142857\n",
      "episode reward mean:  -62.44642857142857\n",
      "episode reward mean:  -62.44642857142857\n",
      "episode reward mean:  -62.44642857142857\n",
      "episode reward mean:  -62.44642857142857\n",
      "episode reward mean:  -62.44642857142857\n",
      "episode reward mean:  -62.44642857142857\n",
      "episode reward mean:  -62.44642857142857\n",
      "episode reward mean:  -62.44642857142857\n",
      "episode reward mean:  -62.44642857142857\n",
      "Game completed with a score of: -91 at time: 189\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "Game completed with a score of: 92 at time: 8\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -60.275862068965516\n",
      "Game completed with a score of: 96 at time: 4\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -57.6271186440678\n",
      "episode reward mean:  -57.6271186440678\n",
      "episode reward mean:  -57.6271186440678\n",
      "episode reward mean:  -57.6271186440678\n",
      "episode reward mean:  -57.6271186440678\n",
      "episode reward mean:  -57.6271186440678\n",
      "episode reward mean:  -57.6271186440678\n",
      "episode reward mean:  -57.6271186440678\n",
      "episode reward mean:  -57.6271186440678\n",
      "episode reward mean:  -57.6271186440678\n",
      "episode reward mean:  -57.6271186440678\n",
      "episode reward mean:  -57.6271186440678\n",
      "episode reward mean:  -57.6271186440678\n",
      "Game completed with a score of: -122 at time: 219\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -58.7\n",
      "episode reward mean:  -58.7\n",
      "episode reward mean:  -58.7\n",
      "episode reward mean:  -58.7\n",
      "episode reward mean:  -58.7\n",
      "episode reward mean:  -58.7\n",
      "Game completed with a score of: -15 at time: 110\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -57.98360655737705\n",
      "episode reward mean:  -57.98360655737705\n",
      "Game completed with a score of: 73 at time: 26\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -55.87096774193548\n",
      "episode reward mean:  -55.87096774193548\n",
      "episode reward mean:  -55.87096774193548\n",
      "Game completed with a score of: 45 at time: 51\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -54.26984126984127\n",
      "episode reward mean:  -54.26984126984127\n",
      "Game completed with a score of: 70 at time: 28\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "Game completed with a score of: 92 at time: 8\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -50.10769230769231\n",
      "episode reward mean:  -50.10769230769231\n",
      "episode reward mean:  -50.10769230769231\n",
      "episode reward mean:  -50.10769230769231\n",
      "episode reward mean:  -50.10769230769231\n",
      "episode reward mean:  -50.10769230769231\n",
      "episode reward mean:  -50.10769230769231\n",
      "Game completed with a score of: -23 at time: 120\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -49.696969696969695\n",
      "episode reward mean:  -49.696969696969695\n",
      "episode reward mean:  -49.696969696969695\n",
      "episode reward mean:  -49.696969696969695\n",
      "episode reward mean:  -49.696969696969695\n",
      "Game completed with a score of: 12 at time: 85\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -48.776119402985074\n",
      "episode reward mean:  -48.776119402985074\n",
      "episode reward mean:  -48.776119402985074\n",
      "Game completed with a score of: 57 at time: 40\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "Game completed with a score of: 97 at time: 1\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "checkpoint saved at /root/ray_results/DQN_LightEnvWrapper_2022-09-15_14-05-24733puria/checkpoint_000601\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "episode reward mean:  -45.130434782608695\n",
      "Game completed with a score of: -371 at time: 466\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -49.785714285714285\n",
      "Game completed with a score of: 86 at time: 13\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -47.87323943661972\n",
      "episode reward mean:  -47.87323943661972\n",
      "episode reward mean:  -47.87323943661972\n",
      "episode reward mean:  -47.87323943661972\n",
      "episode reward mean:  -47.87323943661972\n",
      "episode reward mean:  -47.87323943661972\n",
      "episode reward mean:  -47.87323943661972\n",
      "episode reward mean:  -47.87323943661972\n",
      "episode reward mean:  -47.87323943661972\n",
      "episode reward mean:  -47.87323943661972\n",
      "episode reward mean:  -47.87323943661972\n",
      "episode reward mean:  -47.87323943661972\n",
      "episode reward mean:  -47.87323943661972\n",
      "episode reward mean:  -47.87323943661972\n",
      "episode reward mean:  -47.87323943661972\n",
      "episode reward mean:  -47.87323943661972\n",
      "episode reward mean:  -47.87323943661972\n",
      "episode reward mean:  -47.87323943661972\n",
      "episode reward mean:  -47.87323943661972\n",
      "episode reward mean:  -47.87323943661972\n",
      "Game completed with a score of: -237 at time: 333\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -50.5\n",
      "Game completed with a score of: 74 at time: 25\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -48.794520547945204\n",
      "Game completed with a score of: 77 at time: 21\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -47.0945945945946\n",
      "episode reward mean:  -47.0945945945946\n",
      "Game completed with a score of: 62 at time: 37\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -45.64\n",
      "episode reward mean:  -45.64\n",
      "episode reward mean:  -45.64\n",
      "episode reward mean:  -45.64\n",
      "episode reward mean:  -45.64\n",
      "episode reward mean:  -45.64\n",
      "episode reward mean:  -45.64\n",
      "episode reward mean:  -45.64\n",
      "episode reward mean:  -45.64\n",
      "episode reward mean:  -45.64\n",
      "episode reward mean:  -45.64\n",
      "episode reward mean:  -45.64\n",
      "episode reward mean:  -45.64\n",
      "episode reward mean:  -45.64\n",
      "episode reward mean:  -45.64\n",
      "Game completed with a score of: -147 at time: 242\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -46.973684210526315\n",
      "episode reward mean:  -46.973684210526315\n",
      "Game completed with a score of: 64 at time: 34\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -45.532467532467535\n",
      "episode reward mean:  -45.532467532467535\n",
      "episode reward mean:  -45.532467532467535\n",
      "episode reward mean:  -45.532467532467535\n",
      "episode reward mean:  -45.532467532467535\n",
      "episode reward mean:  -45.532467532467535\n",
      "episode reward mean:  -45.532467532467535\n",
      "Game completed with a score of: -25 at time: 120\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -45.26923076923077\n",
      "Game completed with a score of: 93 at time: 7\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -43.51898734177215\n",
      "episode reward mean:  -43.51898734177215\n",
      "episode reward mean:  -43.51898734177215\n",
      "episode reward mean:  -43.51898734177215\n",
      "episode reward mean:  -43.51898734177215\n",
      "episode reward mean:  -43.51898734177215\n",
      "episode reward mean:  -43.51898734177215\n",
      "episode reward mean:  -43.51898734177215\n",
      "episode reward mean:  -43.51898734177215\n",
      "episode reward mean:  -43.51898734177215\n",
      "episode reward mean:  -43.51898734177215\n",
      "episode reward mean:  -43.51898734177215\n",
      "episode reward mean:  -43.51898734177215\n",
      "episode reward mean:  -43.51898734177215\n",
      "episode reward mean:  -43.51898734177215\n",
      "episode reward mean:  -43.51898734177215\n",
      "episode reward mean:  -43.51898734177215\n",
      "episode reward mean:  -43.51898734177215\n",
      "Game completed with a score of: -212 at time: 307\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -45.625\n",
      "episode reward mean:  -45.625\n",
      "episode reward mean:  -45.625\n",
      "episode reward mean:  -45.625\n",
      "episode reward mean:  -45.625\n",
      "Game completed with a score of: 17 at time: 81\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "Game completed with a score of: 99 at time: 1\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -43.09756097560975\n",
      "Game completed with a score of: 91 at time: 8\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -41.48192771084337\n",
      "episode reward mean:  -41.48192771084337\n",
      "episode reward mean:  -41.48192771084337\n",
      "episode reward mean:  -41.48192771084337\n",
      "episode reward mean:  -41.48192771084337\n",
      "episode reward mean:  -41.48192771084337\n",
      "episode reward mean:  -41.48192771084337\n",
      "episode reward mean:  -41.48192771084337\n",
      "episode reward mean:  -41.48192771084337\n",
      "checkpoint saved at /root/ray_results/DQN_LightEnvWrapper_2022-09-15_14-05-24733puria/checkpoint_000701\n",
      "episode reward mean:  -41.48192771084337\n",
      "episode reward mean:  -41.48192771084337\n",
      "episode reward mean:  -41.48192771084337\n",
      "episode reward mean:  -41.48192771084337\n",
      "episode reward mean:  -41.48192771084337\n",
      "episode reward mean:  -41.48192771084337\n",
      "episode reward mean:  -41.48192771084337\n",
      "episode reward mean:  -41.48192771084337\n",
      "episode reward mean:  -41.48192771084337\n",
      "Game completed with a score of: -213 at time: 308\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -43.523809523809526\n",
      "episode reward mean:  -43.523809523809526\n",
      "episode reward mean:  -43.523809523809526\n",
      "episode reward mean:  -43.523809523809526\n",
      "episode reward mean:  -43.523809523809526\n",
      "episode reward mean:  -43.523809523809526\n",
      "episode reward mean:  -43.523809523809526\n",
      "Game completed with a score of: -12 at time: 109\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "episode reward mean:  -43.15294117647059\n",
      "Game completed with a score of: -338 at time: 433\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -46.58139534883721\n",
      "episode reward mean:  -46.58139534883721\n",
      "episode reward mean:  -46.58139534883721\n",
      "Game completed with a score of: 39 at time: 58\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -45.59770114942529\n",
      "episode reward mean:  -45.59770114942529\n",
      "episode reward mean:  -45.59770114942529\n",
      "episode reward mean:  -45.59770114942529\n",
      "episode reward mean:  -45.59770114942529\n",
      "episode reward mean:  -45.59770114942529\n",
      "episode reward mean:  -45.59770114942529\n",
      "episode reward mean:  -45.59770114942529\n",
      "episode reward mean:  -45.59770114942529\n",
      "episode reward mean:  -45.59770114942529\n",
      "episode reward mean:  -45.59770114942529\n",
      "episode reward mean:  -45.59770114942529\n",
      "episode reward mean:  -45.59770114942529\n",
      "Game completed with a score of: -125 at time: 220\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -46.5\n",
      "episode reward mean:  -46.5\n",
      "episode reward mean:  -46.5\n",
      "episode reward mean:  -46.5\n",
      "episode reward mean:  -46.5\n",
      "episode reward mean:  -46.5\n",
      "episode reward mean:  -46.5\n",
      "episode reward mean:  -46.5\n",
      "episode reward mean:  -46.5\n",
      "episode reward mean:  -46.5\n",
      "episode reward mean:  -46.5\n",
      "episode reward mean:  -46.5\n",
      "episode reward mean:  -46.5\n",
      "episode reward mean:  -46.5\n",
      "episode reward mean:  -46.5\n",
      "Game completed with a score of: -144 at time: 239\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -47.59550561797753\n",
      "episode reward mean:  -47.59550561797753\n",
      "Game completed with a score of: 61 at time: 35\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -46.388888888888886\n",
      "episode reward mean:  -46.388888888888886\n",
      "episode reward mean:  -46.388888888888886\n",
      "episode reward mean:  -46.388888888888886\n",
      "episode reward mean:  -46.388888888888886\n",
      "episode reward mean:  -46.388888888888886\n",
      "episode reward mean:  -46.388888888888886\n",
      "episode reward mean:  -46.388888888888886\n",
      "episode reward mean:  -46.388888888888886\n",
      "episode reward mean:  -46.388888888888886\n",
      "episode reward mean:  -46.388888888888886\n",
      "Game completed with a score of: -92 at time: 189\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -46.89010989010989\n",
      "episode reward mean:  -46.89010989010989\n",
      "episode reward mean:  -46.89010989010989\n",
      "episode reward mean:  -46.89010989010989\n",
      "episode reward mean:  -46.89010989010989\n",
      "episode reward mean:  -46.89010989010989\n",
      "episode reward mean:  -46.89010989010989\n",
      "episode reward mean:  -46.89010989010989\n",
      "episode reward mean:  -46.89010989010989\n",
      "episode reward mean:  -46.89010989010989\n",
      "episode reward mean:  -46.89010989010989\n",
      "episode reward mean:  -46.89010989010989\n",
      "Game completed with a score of: -100 at time: 197\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -47.46739130434783\n",
      "Game completed with a score of: 84 at time: 15\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -46.053763440860216\n",
      "checkpoint saved at /root/ray_results/DQN_LightEnvWrapper_2022-09-15_14-05-24733puria/checkpoint_000801\n",
      "episode reward mean:  -46.053763440860216\n",
      "episode reward mean:  -46.053763440860216\n",
      "episode reward mean:  -46.053763440860216\n",
      "episode reward mean:  -46.053763440860216\n",
      "episode reward mean:  -46.053763440860216\n",
      "Game completed with a score of: -10 at time: 107\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -45.670212765957444\n",
      "episode reward mean:  -45.670212765957444\n",
      "episode reward mean:  -45.670212765957444\n",
      "episode reward mean:  -45.670212765957444\n",
      "episode reward mean:  -45.670212765957444\n",
      "episode reward mean:  -45.670212765957444\n",
      "episode reward mean:  -45.670212765957444\n",
      "Game completed with a score of: -10 at time: 108\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -45.294736842105266\n",
      "episode reward mean:  -45.294736842105266\n",
      "episode reward mean:  -45.294736842105266\n",
      "episode reward mean:  -45.294736842105266\n",
      "episode reward mean:  -45.294736842105266\n",
      "episode reward mean:  -45.294736842105266\n",
      "episode reward mean:  -45.294736842105266\n",
      "episode reward mean:  -45.294736842105266\n",
      "episode reward mean:  -45.294736842105266\n",
      "episode reward mean:  -45.294736842105266\n",
      "episode reward mean:  -45.294736842105266\n",
      "episode reward mean:  -45.294736842105266\n",
      "episode reward mean:  -45.294736842105266\n",
      "episode reward mean:  -45.294736842105266\n",
      "episode reward mean:  -45.294736842105266\n",
      "episode reward mean:  -45.294736842105266\n",
      "episode reward mean:  -45.294736842105266\n",
      "episode reward mean:  -45.294736842105266\n",
      "episode reward mean:  -45.294736842105266\n",
      "Game completed with a score of: -221 at time: 318\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -47.125\n",
      "episode reward mean:  -47.125\n",
      "episode reward mean:  -47.125\n",
      "episode reward mean:  -47.125\n",
      "episode reward mean:  -47.125\n",
      "episode reward mean:  -47.125\n",
      "episode reward mean:  -47.125\n",
      "Game completed with a score of: -27 at time: 123\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "episode reward mean:  -46.91752577319588\n",
      "Game completed with a score of: -278 at time: 376\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "episode reward mean:  -49.275510204081634\n",
      "Game completed with a score of: -308 at time: 404\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -51.888888888888886\n",
      "episode reward mean:  -51.888888888888886\n",
      "episode reward mean:  -51.888888888888886\n",
      "episode reward mean:  -51.888888888888886\n",
      "episode reward mean:  -51.888888888888886\n",
      "Game completed with a score of: 24 at time: 76\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -51.13\n",
      "episode reward mean:  -51.13\n",
      "Game completed with a score of: 65 at time: 34\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -51.46\n",
      "episode reward mean:  -51.46\n",
      "Game completed with a score of: 60 at time: 39\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "checkpoint saved at /root/ray_results/DQN_LightEnvWrapper_2022-09-15_14-05-24733puria/checkpoint_000901\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "episode reward mean:  -48.72\n",
      "Game completed with a score of: -453 at time: 548\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "episode reward mean:  -53.64\n",
      "Game completed with a score of: -513 at time: 609\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -59.74\n",
      "Game completed with a score of: 98 at time: 2\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "Game completed with a score of: 90 at time: 10\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "Game completed with a score of: 99 at time: 1\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -59.25\n",
      "episode reward mean:  -59.25\n",
      "episode reward mean:  -59.25\n",
      "episode reward mean:  -59.25\n",
      "episode reward mean:  -59.25\n",
      "episode reward mean:  -59.25\n",
      "episode reward mean:  -59.25\n",
      "episode reward mean:  -59.25\n",
      "episode reward mean:  -59.25\n",
      "episode reward mean:  -59.25\n",
      "episode reward mean:  -59.25\n",
      "episode reward mean:  -59.25\n",
      "episode reward mean:  -59.25\n",
      "episode reward mean:  -59.25\n",
      "episode reward mean:  -59.25\n",
      "Game completed with a score of: -156 at time: 253\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -61.44\n",
      "Game completed with a score of: 95 at time: 5\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -60.92\n",
      "episode reward mean:  -60.92\n",
      "episode reward mean:  -60.92\n",
      "episode reward mean:  -60.92\n",
      "episode reward mean:  -60.92\n",
      "episode reward mean:  -60.92\n",
      "episode reward mean:  -60.92\n",
      "episode reward mean:  -60.92\n",
      "episode reward mean:  -60.92\n",
      "episode reward mean:  -60.92\n",
      "episode reward mean:  -60.92\n",
      "episode reward mean:  -60.92\n",
      "Game completed with a score of: -104 at time: 201\n",
      "done is True, resetting environment in wrapper.\n",
      "resetting in wrapper\n",
      "resetting\n",
      "resetting in wrapper\n",
      "resetting\n",
      "episode reward mean:  -62.48\n",
      "episode reward mean:  -62.48\n",
      "episode reward mean:  -62.48\n",
      "episode reward mean:  -62.48\n",
      "episode reward mean:  -62.48\n",
      "episode reward mean:  -62.48\n",
      "episode reward mean:  -62.48\n"
     ]
    }
   ],
   "source": [
    "# !pip install ray==1.11.0\n",
    "import gym\n",
    "import ray.rllib.agents.ppo.ppo as ppo\n",
    "\n",
    "\n",
    "import ray\n",
    "import ray.rllib.agents.dqn as dqn\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "config = dqn.DEFAULT_CONFIG.copy()\n",
    "config[\"preprocessor_pref\"] = \"rllib\"\n",
    "config[\"framework\"] = \"torch\"\n",
    "config[\"num_gpus\"] = 1\n",
    "config[\"num_gpus_per_worker\"] = 1\n",
    "config[\"dueling\"] = False\n",
    "config[\"double_q\"] = False\n",
    "config[\"env\"] = LightEnvWrapper\n",
    "# config[\"env_config\"] = { \"size_env\": 15}\n",
    "config[\"model\"] = { \"fcnet_hiddens\": [64, 64],\n",
    "                    \"fcnet_activation\": \"relu\",\n",
    "    }\n",
    "print(config)\n",
    "trainer = dqn.DQNTrainer(config=config)\n",
    "# Can optionally call trainer.restore(path) to load a checkpoint.\n",
    "\n",
    "avg_rewards = []\n",
    "num_iterations = []\n",
    "\n",
    "for i in range(1000):\n",
    "    # Perform one iteration of training the policy with PPO\n",
    "    result = trainer.train()\n",
    "    #print(pretty_print(result))\n",
    "    print(\"episode reward mean: \", result['episode_reward_mean'])\n",
    "    avg_rewards.append(result['episode_reward_mean'])\n",
    "    num_iterations.append(i)\n",
    "    if i % 100 == 0:\n",
    "        checkpoint = trainer.save()\n",
    "        print(\"checkpoint saved at\", checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_EAmchE5bKAQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "_EAmchE5bKAQ",
    "outputId": "6615c1ed-2651-4c29-ec74-ba7236d2e5ae"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEGCAYAAACO8lkDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9bn48c8zk40kJASQsIRVUURRwLhvwaVaq0W9tm5VWm2pdaN6+2u13la917YuVWurtqXXVlqraNVbrVo3JIq7gqACIruELRDZsm/P749zJkwyk8lkMpMzmXner1deyZz1+eYk88x3O0dUFWOMMSaYz+sAjDHGJB9LDsYYY0JYcjDGGBPCkoMxxpgQlhyMMcaEyPA6gHgYPHiwjhkzJqZ9a2pqyMvLi29ASc7KnB6szOmhJ2VeuHDhdlXdJ9y6lEgOY8aM4cMPP4xp3/LycsrKyuIbUJKzMqcHK3N66EmZRWR9Z+usWckYY0wISw7GGGNCWHIwxhgTIuF9DiLyZ+BMoFJVD3aXDQQeB8YA64BvquoOERHgPuAMoBb4tqouSnSMxpj01NTUREVFBfX19V6HErPCwkKWL18ecZucnBxKSkrIzMyM+ri90SH9MHA/8NegZTcA81T1dhG5wX39E+CrwHj360jg9+53Y4yJu4qKCvr378+YMWNwPpv2PXv27KF///6drldVqqqqqKioYOzYsVEfN+HNSqr6BvBlh8XTgTnuz3OAs4OW/1Ud7wIDRGRYomM0xqSn+vp6Bg0a1GcTQzREhEGDBnW7duTVUNZiVd3s/rwFKHZ/HgFsCNquwl22mQ5EZCYwE6C4uJjy8vKYAqmuro55377KypwerMxdKywspLq6OnEB9YKWlhb27NnT5Xb19fXd+t14Ps9BVVVEun3fcFWdDcwGKC0t1VjG+a7YsocH/vUOY0a3r5xkZ/q55OjRFORE3z7Xl9hY8PRgZe7a8uXLIzbJ9AVdNSsF5OTkMGXKlKiP61Vy2Coiw1R1s9tsVOku3wiMDNquxF2WEKsqq/nX6iZYs6ptWeDxFqMH5XLmIcMTdWpjjAHA7/czadIkmpqayMjI4NJLL+W6667D53Na/d98802uv/56du/ejaoya9YsrrzySgBuueUW7rzzTtatW8eQIUMAyM/Pj0ttyKvk8CwwA7jd/f5M0PKrRWQuTkf0rqDmp7j72iHDyPsyr90njdXbqjn57tdpabWHIBljEq9fv34sXrwYgMrKSi666CJ2797NrbfeypYtW7jooov45z//ydSpU9m+fTunnXYaw4YN45xzzgFg0KBB3H333dxxxx1xjSvhHdIi8hjwDnCAiFSIyOU4SeFUEVkJnOK+BngBWAOsAv4EXJno+IwxJlkMGTKE2bNnc//996OqPPDAA3z7299m6tSpAAwePJg777yTu+66q22fb33rWzz++ON8+WXHcT89k/Cag6pe2Mmqk8Nsq8BViY3IGGNC3fqvpSzbtDuux5w4vICbzzqoW/uMGzeOlpYWKisrWbp0KTNmzGi3vrS0lGXLlrW9zs/P57LLLuO+++7j1ltvjUvcYDOkQ6TugDZjTKq69tprmTNnTlSjlqLl+WilZKXW5WBMWunuJ/xEWbNmDX6/nyFDhjBx4kQWLlzI9OnT29YvXLiQ0tLSdvsMGDCAiy66iAceeCBucVhy6CCVJ8MYY5Lbtm3buOKKK7j66qsREa666iqOPPJIzj33XCZPnkxVVRU33XQTt99+e8i+119/PYcffjjNzc1xicWSQycUqzoYYxKvrq6OyZMntw1lveSSS7j++usBGDZsGI888ggzZ85k165drFu3jocffpgTTzwx5DiDBw/mnHPO4d57741LXJYcjDHGQy0tLRHXn3DCCbz//vsAPPjgg/zyl7/k9NNPp6ioiFtuuaVdP8M999zDPffcE5e4rEO6g0CjkvU5GGOSzZVXXsknn3xCUVFRws9lycEYY0wISw4dWH+0MelF06CZIJYyWnLoRBr8vRiT9nJycqiqqkrpBBF4nkNOTk639rMOaWNM2iopKaGiooJt27Z5HUrM6uvru3zjDzwJrjssOXQgbpd06n6OMMYEZGZmduvpaMmovLy8W7fijpY1KxljjAlhyaED65A2xhhLDp1K5Q4qY4zpiiUHY4wxITxNDiJynYgsFZFPReQxEckRkbEi8p6IrBKRx0Uky4vYrN5gjElnniUHERkBXAuUqurBgB+4ALgDuFdV9wN2AJd7FaMxxqQrr5uVMoB+IpIB5AKbgZOAJ931c4CzezMg65A2xhgQLzteRWQW8AugDngZmAW869YaEJGRwL/dmkXHfWcCMwGKi4sPmzt3bkwxVFdXk5+f3/Z6e10rP3q9jssOzuKEksyYjpnsOpY5HViZ04OVuXumTZu2UFVLw63zbBKciBQB04GxwE7gH8Dp0e6vqrOB2QClpaVaVlYWUxzl5eUE77txZx28/hoTDphA2eEjYzpmsutY5nRgZU4PVub48bJZ6RRgrapuU9Um4GngWGCA28wEUAJs9CI4e9iPMSadeZkcvgCOEpFccZ7NeTKwDJgPnOduMwN4xqP4jDEmbXmWHFT1PZyO50XAJ24ss4GfANeLyCpgEPBQb8ZlD/sxxhiPb7ynqjcDN3dYvAY4woNwjDHGuLweypp0bCirMcZYcuiUtSoZY9KZJYcOBKs6GGOMJYdOWIe0MSadWXIwxhgTwpJDB9YhbYwxlhw6ZTOkjTHpzJJDB1ZxMMYYSw6dsg5pY0w6s+RgjDEmhCWHjtx2Jas4GGPSmSUHY4wxISw5dGAzpI0xxpJD56xH2hiTxiw5dGCT4IwxxuPkICIDRORJEflMRJaLyNEiMlBEXhGRle73Ii9is3qDMSadeV1zuA94UVUnAIcCy4EbgHmqOh6Y5742xhjTizxLDiJSCJyA+xhQVW1U1Z3AdGCOu9kc4Oxejas3T2aMMUlK1KOOVxGZjPPM6GU4tYaFwCxgo6oOcLcRYEfgdYf9ZwIzAYqLiw+bO3duTHFUV1eTn5/f9np3o3Lta7V868AsThmdGdMxk13HMqcDK3N6sDJ3z7Rp0xaqamnYlarqyRdQCjQDR7qv7wP+B9jZYbsdXR3rsMMO01jNnz+/3euq6gYd/ZPn9OG31sZ8zGTXsczpwMqcHqzM3QN8qJ28r3rZ51ABVKjqe+7rJ4GpwFYRGQbgfq/0Iji1oazGmDTmWXJQ1S3ABhE5wF10Mk4T07PADHfZDOAZD8Izxpi0luHx+a8B/i4iWcAa4Ds4CesJEbkcWA98szcDsg5pY4zxODmo6mKcvoeOTu7tWDrqbqPS7DdW88ziTSHLZ54wjumTR8QnKGOM6SVez3NIOrHOkH74rXXsqmtiWGFO29eabTXM/8yTLhNjjOkRr5uVklZ3+qMrd9ezaVc9//W1A/nu8ePalp98dzmNLa0JiM4YYxLLag4ddPeurF/WNHLj058AcOjI9tMxsjL8NDbbqCdjTN9jNYdORPuWft7v32bN9hpOO6iYKR2Tg1+s5mCM6ZMsOfTAztpG1myv4ezJw/nNBVNC1mdl+GhqtuRgjOl7rFmpo260Kq2qrAbodDRSpt9Hk9UcjDF9kCWHTkQzQ3rL7noAhg3ICbs+K8MXtllpZ20jX1TVUt3Q3LMgjTEmQaxZqYPuDGXdsstNDgX9wq7P9Pto7NCsVFXdwDG3v0ZDcyvDC3N464aTEHvCkDEmyVhy6IGVW6spys2koF/4X2NWho+G5lYu+tO7VOyoQwQOKO5PQ3Mrx48fzIKV23ns/Q0cMLQ//XMyGD8k3xKFMSYpdJocROTcSDuq6tPxD6dv+WzrHiYOL+j0DT3L7+OLL2tZu72G0tFFrNiyh5ertgJwzUnjWbByOz/9v0/atv+PqSWcsP9gTjmwmLxsy9vGGO9Eegc6y/0+BDgGeM19PQ14G0jJ5BDubX53fRN3v7SC2sYWACYMK+Dy48ayp66JkUXhm5TASQ4trU7fxTUnj+d381by4fodDM7P4oixA3nvpydTsaOOXXWN/OCRRTy1qIKnFlVw81kT+c6xYxNRPGOMiUqnyUFVvwMgIi8DE1V1s/t6GPBwr0TnoeD+6A/Wfsmcd9YzOD+b+qYWnlm8icuPG0tNYzN5WZ3n18yMvalm1MBcMv1O///oQXkAFBfkUFzgdGYv/NmpfFndyBm/XcD6qtoElMgYY6IXTdvFyEBicG0FRiUoHs+FayKqcWsMc2ceybNLNvPbeStpbVVqG1rIzfZ3eqzS0QN5dVkl5x8+krGD8/hGaQm1TS38/ltTQ7bNz84gPzuDkQNz+es765j7wRft1vtE+NW5k+wmfsaYXhFNcpgnIi8Bj7mvzwdeTVxIyUGD5kjXukNOc7MyyM5wPv03trR2WXM4e8oIzp6y98383KklnDu1JOJ5bzrjQBas3Bay/KE317J0025LDsaYXtFlclDVq0XkHOAEd9FsVf2/xIaVXAJ9DblZfrLcpqHddU20KnHvOD5u/GCOGz84ZPnjH26goaklrucyxpjORHxnExE/sFRVJwAJSQjuOT4ENqrqmSIyFpgLDAIWApeoamMizh02Hvd7cJ9DbWNQzSHTSQ6vLHdGHeVHaFaKp+wMH/VNNtvaGNM7Is6QVtUWYIWIJLKPYRawPOj1HcC9qrofsAO4PIHnjkptYwsZPiErw9dWc/j5M0sBOHXi0F6JISfTT32z1RyMMb0jmttnFAFLRWSeiDwb+IrHyUWkBPga8L/uawFOAp50N5kDnB2Pc0Uf096fVZU/vL6aB8tXt/U1ZLnfW1qVc6eMYGhh+FtnxFtOhp96a1YyxvSSaBrMf5bA8/8G+DHQ3309CNipqoGbDlUAYXtgRWQmMBOguLiY8vLymAKorq5ut2+D+/yF1atX82TNem5/ow6A/9jPT3l5OSs3O6ENyBbOGrIj5vN2V2N9HZu21sblfB3LnA6szOnByhw/0XRIvx73swIiciZQqaoLRaSsu/ur6mxgNkBpaamWlXX7EACUl5cTvG9dYwu8+iLj9t2X8eMGwRtv8dCMUk4+sBiALxdVwJIlTJs4nGnTJsd0zlg8uOIdBCgrO7rHx+pY5nRgZU4PVub46TI5iMhRwO+AA4EswA/UqGpBD899LPB1ETkDyAEKgPuAASKS4dYeSoCNPTxPTFRhR63TD16Ul9W2vM5t2snJ7N0b2mZn+Ni2p4GVW/d0us3AvCwG5Wf3YlTGmFQVTbPS/cAFwD+AUuBSYP+enlhVbwRuBHBrDj9S1YtF5B/AeTgjlmYAz/T0XLGa/1klAANzg5JDYyA59M4opYDCfpksWLmdU+99o9NtcrP8fPTzU8nO6N3YjDGpJ6pB+qq6SkT87uilv4jIR7hv7AnwE2CuiNwGfAQ8lKDzhBXcIf25+yk9uNP55AOLue355XzjsJG9GRY/O3Mipx/c+cio8hXbeHJhBfWNrZYcjDE9Fk1yqBWRLGCxiNwJbCbODwlS1XKg3P15DXBEPI8fC0XZWdvEaQcVt6sljB2cx7rbv9br8RQX5HDmIcM7Xb+ztoknF1bQ0NICZPZeYMaYlBTNm/wl7nZXAzXASOA/EhlUsviyppGBQf0NySww/6Kppesn2BljTFeiqTnshzOqaDdwa4LjSRpbd9VTuaeBoty+kRwCd4BtarZZ1MaYnoum5nApsERE3hWRu0TkLBEpSnRgXvtw/Q4ARg/K9TiS6GS21RwsORhjei6aeQ4zAERkOM4oogeA4dHs2xcFOqTdZ/Rw2Oi+kQcDyaHBag7GmDiIZp7Dt4DjgUnAdpyhrQsSHJfnWt3s0Fee6ZxlNQdjTBxF8+n/N8Bq4A/AfFVdl9CIPCbufVlb3duy+vpKcsiwDmljTPx02eegqoOBy3BmMf9CRN4Xkb8lPDKPtbQlB48DiZL1ORhj4qnL5CAiBTiPBR0NjAEKgZR9B2rrc2jtWzWHTL8TZ6P1ORhj4iCaZqU3g77uV9WKxIaUHNpqDn2k6hCoOTRazcEYEwfRjFY6BEBEclW1NvEhJYdW9z22j+SGtj6HR9/7grdXbe90O59PmOCzBGKMiSya0UpH49zfKB8YJSKHAt9X1SsTHZwXArmgr3VIDyvMYdzgPJZU7GRJxc5Ot9tZ28TZ+2Vyfi/GZozpe6IdrXQa8CyAqi4RkRMSGlUSaOljfQ79czJ57UdlXW43/qYXsG4JY0xXorqBnqpu6LAoZZ9XGZjXEJgE11ealaKV6ffR3GrDXY0xkUVTc9ggIscAKiKZwCxgeWLD8l5fa1aKliUHY0w0oqk5XAFchfMs543AZCAl+xuC9bVmpWhl+n00W24wxnQhmklw21X1YlUtVtUhwDXAD3p6YhEZKSLzRWSZiCwVkVnu8oEi8oqIrHS/9+rNjUI6pHv3aaAJl+UXbLSrMaYrnb71uW/es0XkORG5XETyROTXwApgSBzO3Qz8p6pOBI4CrhKRicANwDxVHQ/Mc1/3ur42CS5amRnWrGSM6Vqkz8V/BTYBvwMOBj7EaVo6RFVn9fTEqrpZVRe5P+/B6ccYAUwH5ribzQHO7um5uqPjXVlTLjlYs5IxJgqROqQHquot7s8vicg3gItVNe6NEiIyBpgCvAcUq+pmd9UWoDje54tGYIZ0iuUGJzmk7FgzY0y8RByt5Lb3B94eq4BCccd6quqX8QhARPKBp4Afquru4Ftkq6qKSNjPuSIyE5gJUFxcTHl5eUznr66uDrtvi9sw/+aCN8hIofGs9bV1iK8l5t9XX9XZdU5lVub0kKgyR0oOhcBC9iYHgEXudwXG9fTk7tDYp4C/q+rT7uKtIjJMVTeLyDCgMty+qjobmA1QWlqqZWVlMcVQXl5OyL4vPt92Z8FpZWX4Uyg53L/8bWqrd4WWOcWFvc4pzsqcHhJV5k6Tg6qOifvZgrg1kIeA5ap6T9CqZ4EZwO3u92cSGUdnNEUnwbWosqyqlVPveb3d8twsPw9cPJWSor7xWFRjTGJ5+ajPY4FLgE9EZLG77Kc4SeEJEbkcWA9806P4gL7zJLhobd5ZD8CQgmwK+2UCsKe+mQUrt7N0025LDsYYwMPkoKpv0r7JKtjJvRlLZ1KpOSngqpP24/EFy3loxuHkZPoBWF9Vw4l3lVNd3+xxdMaYZOFlzSFpiTjNSimYG7jkqNGMrF/blhgA8rKdP4OaRksOxhhHVPN/ReQ4EfmO+/M+IjI2sWElh1RrUupMvpsc9ljNwRjjiuZ5DjcDpcABwF+ATOARnD6DlJaKNYdwsjN8ZPqFjTvrWLe9JvbjZPoYVtgvjpEZY7wSTbPSOTgT1AKzmTeJSP+ERuUxwRmr60+TmoOIUJSbxaPvfcGj733Ro2M9cvmRHDd+cJwiM8Z4JZrk0Bg8GU1E8hIcU9JItVtnRPLQjMNZtW1PzPtv29PAL1/4jO3VDXGMyhjjlWiSwxMi8kdggIh8D7gM+FNiw/KWuD3SaZQbmFRSyKSSwpj3X19Vwy9f+KztVufGmL6ty+Sgqr8WkVOB3Tj9Dj9X1VcSHlkS8KVLp0McBGpZgXtSGWP6tqiGsrrJIC0SQrB06XOIhwy/mxys5mBMSohmtNIenP7ZYLtwbuH9n6q6JhGBeSmQEtJlKGs8BBKpJQdjUkM0NYffABXAozjvmxcA++KMXvozUJao4LxmrUrRCzTBtVqzkjEpIZpJcF9X1T+q6h5V3e3eDfU0VX0c6NVHePaWQIUhnUYr9VTgtubNLZYcjEkF0SSHWhH5poj43K9vAvXuupR+J0jFeyslitUcjEkt0SSHi3HunloJbHV//paI9AOuTmBsnpFO7wdoOhOoOVifgzGpIZqhrGuAszpZ/WZ8w0kuvqjuPGVgbxNcsyUHY1JCNKOVcoDLgYOAnMByVb0sgXElBatBRC/QBNdqycGYlBDNZ+O/AUOB04DXgRIg9vss9AVuTrD+6Oj5bRKcMSklmuSwn6r+DKhR1TnA14AjExsWiMjpIrJCRFaJyA2JPl/YGLw4aR/l8wki1udgTKqIJjk0ud93isjBQCEwJHEhgYj4gQeArwITgQtFZGIiz9nu/Hvj6K1TpgS/iCUHY1JENMlhtogUAf8FPAssA+5IaFRwBLBKVdeoaiMwF5ie4HOGsNTQPX6fWLOSMSkiYoe0iPiA3aq6A3gDGNcrUcEIYEPQ6wo6NGWJyExgJkBxcTHl5eUxnai6ujpk39bWVgBq62pjPm4yC1fmuNBW1q3/gvLyrfE/dg8lrMxJzMqcHhJV5ojJQVVbReTHwBNxP3MPuTO1ZwOUlpZqWVlZTMcpLy+n477+ef+mqbWVvNzckHWpIFyZ4yFr/ksMH1FCWdlBcT92TyWqzMnMypweElXmaJqVXhWRH4nISBEZGPiKeyTtbQRGBr0ucZf1Kutz6B6/X2woqzEpIpob753vfr8qaJmS2CamD4DxIjIWJylcAFyUwPO1E5jfYKmhe/xifQ7GpIpoZkiP7Y1AOpyzWUSuBl4C/MCfVXVpb8dhFYfu8flstJIxqSKaGdK5wPXAKFWdKSLjgQNU9blEBqaqLwAvJPIcnQkkBZsh3T0ZlhyMSRnR9Dn8BWgEjnFfbwRuS1hEScRqDt3jE6Gl1esojDHxEE2fw76qer6IXAigqrViPbUmjAy/UL6ikm/84e29tS5x+m6Ca2MiTiK57LgxnDSh2LN4jTGdi6bm0OjenlsBRGRfoCGhUXnMZkjH5vzDR3LA0P5k+Hz4fM5dbQXnD6dVobUVmltbaWpp5f11X/Lcks1eh2yM6UQ0NYdbgBeBkSLyd+BY4NsJjClpWGronivL9uPKsv2i2vYr975OXVNLgiMyxsQqmtFKL4vIQuAonPfLWaq6PeGReShQY7CKQ+L0y/RbcjAmiUUzWulfwKPAs6pak/iQkoclh8TJyfRT12jJwZhkFU2fw6+B44FlIvKkiJznPgAo5dlQ1sTpl2U1B2OSWZfJQVVfV9UrcWZE/xH4Js7zpFPW3g5pT8NIablZVnMwJplF0yGNO1rpLJxbaUwF5iQyqGRhuSFxcjL91Da2sKe+qW2ZT4S87Kj+JI0xCRZNn8MTOM9XeBG4H3hdVVN7qpNVHRKuf3YGG3fWMemWl9stv+u8Q/hG6chO9jLG9JZoPqY9BFyoqi0AInKciFyoqld1sV+fZ6khcb53wjhGDsxtt+y255fz/578mAfLV7dbLsAPT92frx86vBcjNCa9RTOU9SURmeLOkP4msBZ4OuGRecgqDolXUpTLd49vf2Pf255fDsDBIwrbJeZXl2/ljc+3WXIwphd1mhxEZH/gQvdrO/A4IKo6rZdi85zlBm/87sIp7V5/5d7X2/VNGGMSL1LN4TNgAXCmqq4CEJHreiWqJGG3z0gO/XMy2VPf7HUYxqSVSMnhXJyH7MwXkReBuaTJh+m2GdIex5Fu/nX1cWHnPvTPyaCqutGDiIxJX50mB1X9J/BPEckDpgM/BIaIyO+B/1PVlzvbtysichfO0NhGYDXwHVXd6a67EbgcaAGuVdWXYj1PT1nFoXdNKikMu7x/TiYL1+/ghqc+jvnYmzY38GJV7Pt3ZvrkERy976C4H9cYr0XTIV2Dc/uMR0WkCPgG8BMg5uQAvALc6D7x7Q7gRuAnIjIRp7ZyEDAc5/nV+wdGSvUWe9hPcjl230F8sPZL5q+Ife5lQ0MLK3bHd+7m9upGdtc3WXIwKalbM45UdQcw2/2KWYdax7vAee7P04G5qtoArBWRVThzLN7pyfliZrkhKVxwxCguOGJUj45RXl5OWVlZfAJyfeXe12lN7Rk/Jo0lw3TUy3BGQgGMwEkWARXushAiMhOYCVBcXEx5eXlMJ6+urg7Zt6nJGRmze9fOmI+bzMKVOdUlosy1NbVUal3S/i7tOqeHRJU5YclBRF4FhoZZdZOqPuNucxPQDPy9u8dX1bYaTGlpqcb6qTDcJ8qsN16mpqmJogFFlJUdFdNxk1kiPkUnu0SUuf+SBQwq6kdZWWlcjxsvdp3TQ6LKnLDkoKqnRFovIt8GzgROVtXAU+k3AsH3Tihxl3nCOqRNJALs/dM1JrVEc8vuuBOR04EfA19X1dqgVc8CF4hItoiMBcYD73sQn/u9t89s+hKfz3n8qTGpyKs+h/uBbOAV9434XVW9QlWXujf6W4bT3HRVb49UCmajlUwkPhGrOZiU5UlyUNVOHzSsqr8AftGL4YSweyuZaAhWczCpy5NmJWNSgYjQajUHk6IsOURg91Yykfjsz8OkMEsOYeydIW1M53xWczApzJJDBFZxMJGIYDOkTcqy5BCW3ZXVdE1EUKzmYFKTJYcIrM/BROITG61kUpclhwgsNZhIBJvnYFKXJYcw2jqkLTuYCHw+sNxgUpUlh4gsO5jO2Wglk8osOYRhM6RNtCw1mFRlySECyw0mEqfm4HUUxiSGJYcIrOZgIvGJ3bLbpC5LDmHYM6RNNETEOqRNyrLkEIHVHEwkzjwHyw4mNVlyCCNQY7DkYCIR63MwKczT5CAi/ykiKiKD3dciIr8VkVUi8rGITPU0PmtWMhHYY0JNKvMsOYjISOArwBdBi7+K82jQ8cBM4PcehLa3xmC5wUTgsz4Hk8K8rDnci/Mc6eB/r+nAX9XxLjBARIZ5Eh2WG0xkzjOkLTuY1OTJY0JFZDqwUVWXdLi53QhgQ9DrCnfZ5jDHmIlTu6C4uJjy8vKYYqmurg7Zt6GhAYBtlZUxHzeZhStzqktEmbdtq6empjVpf5d2ndNDosqcsOQgIq8CQ8Osugn4KU6TUsxUdTYwG6C0tFTLyspiOk55eTkd9815Zx7U11NcXExZ2ZSehJmUwpU51SWizE9uWkRVy+6k/V325nV+c+V2bn9xOS0dnm9RkJPBn2aUUpCT2Stx2N92/CQsOajqKeGWi8gkYCwQqDWUAItE5AhgIzAyaPMSd5knbLSSicT6HPZ6edkWPt9azYn779O2bFdtE++t/ZKlG3dz9L6DPIzOxKLXm5VU9RNgSOC1iKwDSlV1u4g8C1wtInOBI4FdqhrSpJRogaYuyw0mEkmzGdKrKqtZumlXyPIDhvZn44469t0nnz9dWtq2fH1VDQHaUUAAABI9SURBVCfeVc6/P93MY+9/wbqqmk6PnZeVwQMXT2VgXlZCYjfd50mfQwQvAGcAq4Ba4DteBmMP+zGRpNu9la5+dBGfbdkTsnzUwFxys/yUFOW2Wz6ssB/9szP46zvrAThoeAFD+meH7L+7vpl31lTx6cZdnBBU8zDe8jw5qOqYoJ8VuMq7aNqz1GAikSSbIb11dz2vfVbZ1tT1+YYmNr33RbttinIzOf3god3+4NPSqqzZVsOFR4ziu8ePbVs+5+11PPLuerIz/Bw1rn3TUVaGjzd+PI2qmkZys/wMH9Av7LFXVe7hlHveYEdtY7di6kpTSyufbQ5NZlkZPvYvzrcPf13wPDkkNfvbMREkW5/Dg/NXMcf9lN5m6Sch25X/qIwxg/NCli/ZsJP/fm4ZzUHVoW2769m0q568LD+NLa2Uji5i333y29ZPHFZAq0JdUwsjwrz5F+VlUdRFU9GAXGf9rLmL+d1rq9qWD87P4i/fPoJ+Wf6I+wfsqmvi423NvPncMj7dtIvlm/ewq64p7LazLzmMrxwUbryMCbDkEIHNkDaRJNsM6e01jYwelMsT3z8agLfffptjjjmmbf1bq7Zz/RNL2NnJG+bzn2xmyYadHLvfYMCZgLRkw04ATj94GGMG5XLWocPb7XPiAftwxqShtLbCKROLY4p7QL+9I5kG52cxKC+b3fVNLFi5nfNnv0Ne1t63qXfWVLVLQsfuN6gtpv/+1zJWVjYAaxlakMOBw/pz/Ph9OKC4f9v2Lap8/28LWbFlD0eMHcjnW6vDxuT3waQRA8jKSN87DFlyCMMeE2qikWx9DrtqmxiUl0VxQQ4ARTm+tp+Btj6B9VU1jCxq/ym/uVWZ/cYa9i/OZ85lR7QtH3/TCzS1KHeedwh+X+g/xLDCfjx48WE9ijvD7+OMSUPZuLOe2Zc6w14bm1u57onFbNvdQIv7S170xQ4AhhRks+8++SxYuY0nPqzgiQ8r2h3vuWuO46DhBZ02Gw0tyOHuVz7n7lc+jxjXT8+YwMwT9u1R2foySw4RWG4wkfh8oEn0LLhddU0Mzu+8CSc/2/l3nzV3cafbnNihQ/jfs46nobk1bGKIp44JJivDxwMXtb+12h9eX83t//6Mn585kSmjiqhrbGHZ5l2oOrWc255fzuT+tRw8ojDiuX517iQWfbEDnwgThxe0/V6CXfXoItZur+0yblVlT0Nzl9sJ0L+X5nrEiyWHMKzmYKKTPDWHJxdW8MnGXZw7ZUSn2wS/Cf739IParfv5M0sBOPOQ9s1G+w3pT7K44sR9ueSo0eS55eiX5eew0QPb1j9z1bFRzRSeNmEI0yYMibjN8MJ+LNmwk8c/+CLs+kNHDmDC0AJ+9e/PmP3Gmqji/3+nHcBV0/aLattkYMkhAutzMJF4/SS4V5dtZeueep5aWNE2xPSKss6bQXKznY7dgXlZXHr0mHbrAskhXEd1MskL8yk/ESYM7c/TH23kJ0+FdugDHDyigOeuOZ6lm3YxamAulx49OuLxHpi/ilWV4fs3kpUlhwis5mAi8XK00vqqGr771w/dOGDaAUO4+KhR7F/c+Sf9QXlZfPe4sZxXWtLpNoX9+lbTR6Lced4h/Oi0A8Kuu+ulFSxYuY3qhmbWba/loOEFfPf4cRGP9+TCCqqjaH5KJpYcwrCH/ZhoeDnPoarGmRPw628cyqkHFlOY2/WbuojwX2dODLvu6SuPoXJ3fVxj7Msy/L5O52WMGphLVU0jZ/52ARt31nHiAV1P3MvPzqDGkkMqsexgOuflaKU99c4bzdjBeVElhq5MHVXU42Oki5EDc1GFdVVOh/XXJnX9VIG87Iy4T/JLNEsOYViHtImGl/dW2u3OVSjIsX/h3jZ98nBGDOiHCBw+ZmBUI7nyczJYV1XD5l11XW4rCMUF2Z7P4La/rAgsN5hIBO/6HD51b4BXYH0EvS7T7+v2XWYH9MtkfVUtR//qtai2T4aRTZYcIrCag4nE52Gfw2vLKwHrQO4rrjlpPIeUFEb1YeIXLyynYkfXNYxEs+QQRiAn+Cw7mAh8PvFkCty67TWsrKxm+uTh5GRGd98h462hhTmcf/ioqLZ9sHw1dY3ed15bcojAUoOJRHDuVrptT0OX22Zl+OL2Kf+255cDzvBVk3r6Zfqpa2rxOgxLDuG0PezHag4mguwMHw3NrRz+i1ej2v4fVxzN4WMGdr2h69ONu9hWvTfxDM7LZlJJIRU7ajlizEDOjjAb2vRd/bL81DamcXIQkWtwnt3QAjyvqj92l98IXO4uv1ZVX+rt2JLpTpsmeV1y9BiKC3O6HM5aVd3Ab15dyaad0bcj76ptYvoDb7XddA6cPrC/f/dIPtuyp8sZuabv6pfppz5daw4iMg2YDhyqqg0iMsRdPhG4ADgIGA68KiL7q2qv/qa+d8I43l5dFXJ7YmOC7dM/m4uP7PpNeuPOOn7z6koamlqjPnbFzlpaWpUbvjqBI8cO5Isva5k1dzEX/ek9wBlOaVJTVoaPdz6v4t+f7H1Cst8nHD9+n6ifbREPXtUcfgDcrqoNAKpa6S6fDsx1l68VkVXAEcA7vRncxUeOjuqf3phoZPmdZwI0NEf/Gadyt9OcdMTYgUwZVcTkkQPYUdNIVU0jE4YWtLvhnEkti9Y7tyb/wd8XtVt+81kT+c6xY8PtkhDiRROKiCwGngFOB+qBH6nqByJyP/Cuqj7ibvcQ8G9VfTLMMWYCMwGKi4sPmzt3bkyxVFdXk5+f3/WGKcTK3Ltqm5Qr59Vy4YQsThsTXad0+YYmHl7ayN0n9mNQv9geOGPXuW/6cEszn1a1cPKovX8rN79dx+B+woUTnFuy+wUOHOQn0yc9KvO0adMWqmppuHUJqzmIyKtAuOfw3eSedyBwFHA48ISIRL5zVQeqOhuYDVBaWqplZWUxxVleXk6s+/ZVVube1dDcAvNepGT0WMrKopvY9NErnyPLVvL1r5SR6Y8tOdh17pvKwiz72VvPU1mr3Ldo7wCFX54ziYuOHJWwMicsOajqKZ2tE5EfAE+rU215X0RagcHARmBk0KYl7jJj+qy9zUqR+xw+qdjF26u3A7Bg5TYG5WXHnBhMarnxqxOo2FHH+Yc7b48Xzn6X5z7eRHNrK407WsImlJ7yqs/hn8A0YL6I7A9kAduBZ4FHReQenA7p8cD7HsVoTFyICFkZPhrDJIf6phZueXYpO2ubeHHplnbrTurigTQmfXz/xPbP6ThsTBHlK7bx9uoqzhibmFnyXiWHPwN/FpFPgUZghluLWCoiTwDLgGbgqt4eqWRMIjhzIlpobmnly5q9d+dcva2GuR9sYOTAfhw4rIDrThnPceMHA5CTYbOfTXj/e2kpu9ybL37w7tsJOYcnyUFVG4FvdbLuF8AvejciYxIrMGHu2rkf8cInW0LW/+2yI5P+KWwmeWT4fQzKzwYgJyMxk3VthrQxvSA7w8+/lmyivqmF48cP5vSDh7JyazXPLN5IcUEOI4rCP1jGGK9YcjCmF3z/xHG8u6YKnwhXn7QfE4YWAHDL1w/yODJjwrPkYEwvuPToMVx69BivwzAmajZOzhhjTAhLDsYYY0JYcjDGGBPCkoMxxpgQlhyMMcaEsORgjDEmhCUHY4wxISw5GGOMCeHJw37iTUS2Aetj3H0wzh1h04mVOT1YmdNDT8o8WlX3CbciJZJDT4jIh509CSlVWZnTg5U5PSSqzNasZIwxJoQlB2OMMSEsObjPoU4zVub0YGVODwkpc9r3ORhjjAllNQdjjDEhLDkYY4wJkdbJQUROF5EVIrJKRG7wOp54EZGRIjJfRJaJyFIRmeUuHygir4jISvd7kbtcROS37u/hYxGZ6m0JYiMifhH5SESec1+PFZH33HI9LiJZ7vJs9/Uqd/0YL+PuCREZICJPishnIrJcRI5O5essIte5f9OfishjIpKTitdZRP4sIpUi8mnQsm5fVxGZ4W6/UkRmdCeGtE0OIuIHHgC+CkwELhSRid5GFTfNwH+q6kTgKOAqt2w3APNUdTwwz30Nzu9gvPs1E/h974ccF7OA5UGv7wDuVdX9gB3A5e7yy4Ed7vJ73e36qvuAF1V1AnAoTvlT8jqLyAjgWqBUVQ8G/MAFpOZ1fhg4vcOybl1XERkI3AwcCRwB3BxIKFFR1bT8Ao4GXgp6fSNwo9dxJaiszwCnAiuAYe6yYcAK9+c/AhcGbd+2XV/5Akrcf5iTgOcAwZk1mtHxegMvAUe7P2e424nXZYihzIXA2o6xp+p1BkYAG4CB7nV7DjgtVa8zMAb4NNbrClwI/DFoebvtuvpK25oDe//QAircZSnFrUpPAd4DilV1s7tqC1Ds/pwKv4vfAD8GWt3Xg4Cdqtrsvg4uU1t53fW73O37mrHANuAvbnPa/4pIHil6nVV1I/Br4AtgM851W0jqX+eA7l7XHl3vdE4OKU9E8oGngB+q6u7gdep8lEiJccwiciZQqaoLvY6ll2UAU4Hfq+oUoIa9TQ1Ayl3nImA6TlIcDuQR2vSSFnrjuqZzctgIjAx6XeIuSwkikomTGP6uqk+7i7eKyDB3/TCg0l3e138XxwJfF5F1wFycpqX7gAEikuFuE1ymtvK66wuBqt4MOE4qgApVfc99/SROskjV63wKsFZVt6lqE/A0zrVP9esc0N3r2qPrnc7J4QNgvDvSIQunY+tZj2OKCxER4CFguareE7TqWSAwYmEGTl9EYPml7qiHo4BdQdXXpKeqN6pqiaqOwbmOr6nqxcB84Dx3s47lDfweznO373OfrlV1C7BBRA5wF50MLCNFrzNOc9JRIpLr/o0HypvS1zlId6/rS8BXRKTIrXV9xV0WHa87XTzu8DkD+BxYDdzkdTxxLNdxOFXOj4HF7tcZOO2t84CVwKvAQHd7wRm5tRr4BGc0iOfliLHsZcBz7s/jgPeBVcA/gGx3eY77epW7fpzXcfegvJOBD91r/U+gKJWvM3Ar8BnwKfA3IDsVrzPwGE6/ShNODfHyWK4rcJlb/lXAd7oTg90+wxhjTIh0blYyxhjTCUsOxhhjQlhyMMYYE8KSgzHGmBCWHIwxxoSw5GD6DBEZJCKL3a8tIrIx6HVWF/uWishvozjH23GKte18IlImIsfE47ju8caIyEXhzmVMvNhQVtMnicgtQLWq/jpoWYbuvcdO0ggXaxT7dFoWESkDfqSqZ8YnQmNCWc3B9Gki8rCI/EFE3gPuFJEjROQd90Z0bwdmD7uf3gPPebjFvV9+uYisEZFrg45XHbR9uex9VsLf3Vm5iMgZ7rKF7n30nwsTV5mIPOfe+PAK4Dq3hnO8iOwjIk+JyAfu17FBcf1NRN4C/ubWEBaIyCL3K1D7uB043j3edR3KNlBE/inOff3fFZFDIpVZRPJE5HkRWSLOMxLOj/9VMn1RRtebGJP0SoBjVLVFRAqA41W1WUROAX4J/EeYfSYA04D+wAoR+b069+sJNgU4CNgEvAUcKyIf4tz6+ARVXSsij0UKTFXXicgfCKo5iMijOM8feFNERuHc0uBAd5eJwHGqWiciucCpqlovIuNxZs2W4txcr63m4NYkAm4FPlLVs0XkJOCvOLOow5YZ58Z1m1T1a+6xCiOVx6QPSw4mFfxDVVvcnwuBOe6bqQKZnezzvKo2AA0iUolz++OKDtu8r6oVACKyGOf++tXAGlVd627zGM4DVrrjFGCiWxEBKBDnDroAz6pqnftzJnC/iEwGWoD9ozj2cbjJUFVfc/tpCtx14cr8CXC3iNyBc9uRBd0si0lRlhxMKqgJ+vl/gPmqeo7bpFPeyT4NQT+3EP5/IZptYuEDjlLV+uCFbrIILst1wFacJ7z5gHbbxyCkPKr6uTiPlTwDuE1E5qnqf/fwPCYFWJ+DSTWF7L0t8bcTcPwVwDjZ+zziaNro9+A05QS8DFwTeOHWDMIpBDaraitwCc5jMcMdL9gC4GL3uGXAdu3wLI9gIjIcqFXVR4C7cG75bYwlB5Ny7gR+JSIfkYCasdvkcyXwoogsxHmj3tXFbv8Czgl0SOM+B9ntNF6G02EdzoPADBFZgtNfEKhVfAy0uJ3I13XY5xbgMBH5GKfjuquHyk8C3nebzW4Gbutie5MmbCirMd0kIvmqWu2OXnoAWKmq93odlzHxZDUHY7rve+4n7aU4TT9/9DgeY+LOag7GGGNCWM3BGGNMCEsOxhhjQlhyMMYYE8KSgzHGmBCWHIwxxoT4/5JkCIz9uQZJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(num_iterations,avg_rewards, label=\"DQN\")\n",
    "# plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['temp'], label=\"temporary reward collected\")\n",
    "plt.xlabel(\"Training iterations\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.legend(loc=1)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
