{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8df2bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random, math\n",
    "import numpy as np\n",
    "import arcade\n",
    "from skimage import data, color\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from PIL import Image\n",
    "\n",
    "        \n",
    "# from LightEnvCopy import LightEnv\n",
    "from LightEnvCopy import LightEnv\n",
    "\n",
    "import gym.spaces\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "# Do the math to figure out our screen dimensions\n",
    "SCREEN_WIDTH = 800\n",
    "SCREEN_HEIGHT = 600\n",
    "SCREEN_TITLE = \"Game 1: Let There Be Light!\"\n",
    "\n",
    "SPRITE_SCALING = 0.25\n",
    "\n",
    "# How fast the camera pans to the player. 1.0 is instant.\n",
    "CAMERA_SPEED = 0.1\n",
    "\n",
    "PLAYER_MOVEMENT_SPEED = 7\n",
    "BOMB_COUNT = 5\n",
    "TORCH_COUNT = 1\n",
    "PLAYING_FIELD_WIDTH = 800 #1600\n",
    "PLAYING_FIELD_HEIGHT = 600 #1600\n",
    "REWARD_COUNT = 1 #TF - Add in reward\n",
    "END_GAME = False\n",
    "torch_collected = False\n",
    "\n",
    "#TF Start - Adding in actions for action conversion\n",
    "\n",
    "# COnvenient data structure to hold information about actions\n",
    "Action = namedtuple('Action', 'name index delta_i delta_j')\n",
    "\n",
    "up = Action('up', 0, -1, 0)    \n",
    "down = Action('down', 1, 1, 0)    \n",
    "left = Action('left', 2, 0, -1)    \n",
    "right = Action('right', 3, 0, 1)    \n",
    "\n",
    "index_to_actions = {}\n",
    "for action in [up, down, left, right]:\n",
    "    index_to_actions[action.index] = action\n",
    "# print(index_to_actions[0].name)\n",
    "str_to_actions = {}\n",
    "for action in [up, down, left, right]:\n",
    "    str_to_actions[action.name] = action\n",
    "#TF End - Adding in actions for action conversion\n",
    "\n",
    "\n",
    "class LightEnvWrapper(gym.Env, LightEnv):\n",
    "    \"\"\"Class that wraps the Lights Environment to make it \n",
    "    compatible with RLLib.\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"rgb_array\", \"state_pixels\"]}\n",
    "    \n",
    "    def __init__(self, config: EnvContext):\n",
    "        super().__init__(SCREEN_WIDTH, SCREEN_HEIGHT, SCREEN_TITLE)\n",
    "        self.counting = 0\n",
    "        \n",
    "        self.torch_collected = False\n",
    "        self.torch_collected_count = []\n",
    "        \n",
    "        self.mygame = LightEnv\n",
    "        #The action space is a choice of 9 actions: U/D/L/R/UR/DR/DL/UL/DO NOTHING. Not continuous\n",
    "        #because speed of agent is fixed. Or potentially just 4: U/D/L/R.\n",
    "        self.action_space = Discrete(4)\n",
    "#         self.action_space = config.get(\"parrot_shriek_range\", Discrete(4))\n",
    "        #The observation space is a fixed image of the current game screen - fully observable.\n",
    "        #Can set to a view just around the player using arcade.set_viewport.\n",
    "        #Need obs space to be either 42,42,x or 84,84,x to be compatible with rllib.\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84,84, 4), dtype=np.uint8)\n",
    "        \n",
    "        self.counting = 0\n",
    "\n",
    "    def reset(self):\n",
    "        print(\"resetting in wrapper\")\n",
    "        \n",
    "        if self.torch_collected == 1:\n",
    "            print(\"Torch was collected this episode!\")\n",
    "        else:\n",
    "            print(\"Torch was not collected this episode...\")\n",
    "        self.torch_collected_count.append(self.torch_collected)\n",
    "        print(self.torch_collected_count)\n",
    "\n",
    "        \n",
    "        self.render(drawing=True)\n",
    "        #Resets the state of the environment for a new episode and an initial observation.\n",
    "        obs_mygame = self.mygame.reset(self)\n",
    "        \n",
    "        #Open up the resetted image to verify working correctly.\n",
    "        obs_mygame.show()\n",
    "        \n",
    "        #Convert observation to 84x84 resolution and np array for rllib.\n",
    "        obs = self.convert_observations(obs_mygame)\n",
    "        \n",
    "#         print(\"resetted\")\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        self.counting += 1\n",
    "        \n",
    "        #Making sure an action is chosen, either: 0, 1, 2, 3.\n",
    "        assert action in [0, 1, 2, 3] #0-up,1-down,2-left,3-right.\n",
    "        \n",
    "        #Convert the numeric action to a keyword: up, down, left, right.\n",
    "        actions_myenv = index_to_actions[action].name #returns a word, one of: up/down/left/right\n",
    "#         print(f\"action taken: {actions_myenv}\")\n",
    "        \n",
    "        #Update the window with on_update()\n",
    "        self.render(self)\n",
    "        \n",
    "        #Compute observation extracted from the window (800x600), with reward and done flag.\n",
    "        obs, reward, done, torch_collected = self.mygame.step(self,actions_myenv)\n",
    "        \n",
    "        if torch_collected == True:\n",
    "            self.torch_collected = 1\n",
    "        else:\n",
    "            self.torch_collected = 0\n",
    "\n",
    "        if self.counting % 33 == 0: #33 steps roughly equates to 1 second in game time\n",
    "            print(f\"total score is {self.score} at time: {self.mygame.time_taken_reported(self)}\")\n",
    "        \n",
    "        #Convert observation to 84x84 resolution and np array for rllib.\n",
    "        obs_mygame = self.convert_observations(obs)\n",
    "        \n",
    "        #If the reward has been obtained, reset the environment and start again\n",
    "        if done == True:\n",
    "            print(f\"done is {done}, resetting environment in wrapper.\")\n",
    "            self.reset()\n",
    "        \n",
    "        return obs_mygame, reward, done, {}\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        random.seed(seed)\n",
    "\n",
    "    def convert_observations(self, obs_mygame): #resizing and converting to array for rllib processing\n",
    "        # We normalize and concatenate observations\n",
    "        obs = obs_mygame\n",
    "        obs_resized = obs.resize((84,84))\n",
    "        obsarray = np.array(obs_resized)\n",
    "        return obsarray\n",
    "    \n",
    "    def render(self, mode='state_pixels', drawing=False):\n",
    "        self.mygame.on_update(self, 1/60)\n",
    "        self.mygame.on_draw(self)\n",
    "        test = self.mygame.time_taken_reported(self)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd8f4ad",
   "metadata": {},
   "source": [
    "### Now run the rllib script to train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3d1b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 12:04:29,798\tWARNING deprecation.py:47 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.LightEnvWrapper'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 1, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': True, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.9, 'lr': 0.01, 'train_batch_size': 256, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 1, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 100, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'twin_q': True, 'q_model_config': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': None, 'custom_model': None, 'custom_model_config': {}}, 'policy_model_config': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': None, 'custom_model': None, 'custom_model_config': {}}, 'tau': 0.005, 'initial_alpha': 1.0, 'target_entropy': 'auto', 'n_step': 1, 'replay_buffer_config': {'_enable_replay_buffer_api': True, 'type': 'MultiAgentPrioritizedReplayBuffer', 'capacity': 100000, 'learning_starts': 1500, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'worker_side_prioritization': False}, 'store_buffer_in_checkpoints': False, 'training_intensity': None, 'optimization': {'actor_learning_rate': 0.0003, 'critic_learning_rate': 0.0003, 'entropy_learning_rate': 0.05}, 'grad_clip': None, 'target_network_update_freq': 0, '_deterministic_loss': False, '_use_beta_distribution': False, 'use_state_preprocessor': -1, 'worker_side_prioritization': -1, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=18544)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=18544)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=18544)\u001b[0m C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=18544)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18544)\u001b[0m 2022-09-16 12:04:46,038\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=18544)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18544)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18544)\u001b[0m [False]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18544)\u001b[0m resetting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 12:04:54,088\tINFO trainable.py:160 -- Trainable.setup took 24.292 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-09-16 12:04:54,094\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode  0\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18544)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18544)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18544)\u001b[0m [False, 0]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18544)\u001b[0m resetting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 12:04:57,409\tERROR algorithm.py:2173 -- Error in training or evaluation attempt! Trying to recover.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 2373, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\dqn\\dqn.py\", line 358, in training_step\n",
      "    new_sample_batch = synchronous_parallel_sample(\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\execution\\rollout_ops.py\", line 100, in synchronous_parallel_sample\n",
      "    sample_batches = ray.get(\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\worker.py\", line 2275, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RayOutOfMemoryError): \u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=18544, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000292F931C0A0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 620, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\memory_monitor.py\", line 162, in raise_if_low_memory\n",
      "    raise RayOutOfMemoryError(\n",
      "ray._private.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node DESKTOP-BKAPO4O is used (7.37 / 7.73 GB). The top 10 memory consumers are:\n",
      "\n",
      "PID\tMEM\tCOMMAND\n",
      "18544\t0.74GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "17572\t0.66GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe -m ipykernel_launcher -f C:\\Users\\Tim\\AppData\\Roaming\\j\n",
      "26756\t0.4GiB\tC:\\Users\\Tim\\AppData\\Local\\Microsoft\\OneDrive\\OneDrive.exe /background\n",
      "1564\t0.31GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "22996\t0.15GiB\tC:\\Program Files\\WindowsApps\\Microsoft.Windows.Photos_2022.30070.26007.0_x64__8wekyb3d8bbwe\\Microsof\n",
      "32536\t0.11GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=gpu-process --gpu-preferences=UA\n",
      "22716\t0.11GiB\tC:\\Program Files (x86)\\Overwolf\\OverwolfLauncher.exe -overwolfsilent -silent\n",
      "18304\t0.11GiB\tC:\\WINDOWS\\Explorer.EXE\n",
      "18936\t0.09GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --no-startup-window --win-session-start\n",
      "26804\t0.09GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "\n",
      "In addition, up to 0.0 GiB of shared memory is currently being used by the Ray object store.\n",
      "---\n",
      "--- Tip: Use the `ray memory` command to list active objects in the cluster.\n",
      "--- To disable OOM exceptions, set RAY_DISABLE_MEMORY_MONITOR=1.\n",
      "---\n",
      "2022-09-16 12:04:58,949\tERROR worker_set.py:728 -- Worker 1 is faulty.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 2373, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\dqn\\dqn.py\", line 358, in training_step\n",
      "    new_sample_batch = synchronous_parallel_sample(\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\execution\\rollout_ops.py\", line 100, in synchronous_parallel_sample\n",
      "    sample_batches = ray.get(\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\worker.py\", line 2275, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RayOutOfMemoryError): \u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=18544, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000292F931C0A0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 620, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\memory_monitor.py\", line 162, in raise_if_low_memory\n",
      "    raise RayOutOfMemoryError(\n",
      "ray._private.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node DESKTOP-BKAPO4O is used (7.37 / 7.73 GB). The top 10 memory consumers are:\n",
      "\n",
      "PID\tMEM\tCOMMAND\n",
      "18544\t0.74GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "17572\t0.66GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe -m ipykernel_launcher -f C:\\Users\\Tim\\AppData\\Roaming\\j\n",
      "26756\t0.4GiB\tC:\\Users\\Tim\\AppData\\Local\\Microsoft\\OneDrive\\OneDrive.exe /background\n",
      "1564\t0.31GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "22996\t0.15GiB\tC:\\Program Files\\WindowsApps\\Microsoft.Windows.Photos_2022.30070.26007.0_x64__8wekyb3d8bbwe\\Microsof\n",
      "32536\t0.11GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=gpu-process --gpu-preferences=UA\n",
      "22716\t0.11GiB\tC:\\Program Files (x86)\\Overwolf\\OverwolfLauncher.exe -overwolfsilent -silent\n",
      "18304\t0.11GiB\tC:\\WINDOWS\\Explorer.EXE\n",
      "18936\t0.09GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --no-startup-window --win-session-start\n",
      "26804\t0.09GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "\n",
      "In addition, up to 0.0 GiB of shared memory is currently being used by the Ray object store.\n",
      "---\n",
      "--- Tip: Use the `ray memory` command to list active objects in the cluster.\n",
      "--- To disable OOM exceptions, set RAY_DISABLE_MEMORY_MONITOR=1.\n",
      "---\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 725, in _worker_health_check\n",
      "    ray.get(obj_ref)\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\worker.py\", line 2275, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RayOutOfMemoryError): \u001b[36mray::RolloutWorker.sample_with_count()\u001b[39m (pid=18544, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000292F931C0A0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 620, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\memory_monitor.py\", line 162, in raise_if_low_memory\n",
      "    raise RayOutOfMemoryError(\n",
      "ray._private.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node DESKTOP-BKAPO4O is used (7.38 / 7.73 GB). The top 10 memory consumers are:\n",
      "\n",
      "PID\tMEM\tCOMMAND\n",
      "18544\t0.74GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "17572\t0.66GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe -m ipykernel_launcher -f C:\\Users\\Tim\\AppData\\Roaming\\j\n",
      "26756\t0.4GiB\tC:\\Users\\Tim\\AppData\\Local\\Microsoft\\OneDrive\\OneDrive.exe /background\n",
      "1564\t0.31GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "22996\t0.15GiB\tC:\\Program Files\\WindowsApps\\Microsoft.Windows.Photos_2022.30070.26007.0_x64__8wekyb3d8bbwe\\Microsof\n",
      "22716\t0.11GiB\tC:\\Program Files (x86)\\Overwolf\\OverwolfLauncher.exe -overwolfsilent -silent\n",
      "18304\t0.11GiB\tC:\\WINDOWS\\Explorer.EXE\n",
      "18936\t0.1GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --no-startup-window --win-session-start\n",
      "32536\t0.09GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=gpu-process --gpu-preferences=UA\n",
      "26804\t0.09GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "\n",
      "In addition, up to 0.0 GiB of shared memory is currently being used by the Ray object store.\n",
      "---\n",
      "--- Tip: Use the `ray memory` command to list active objects in the cluster.\n",
      "--- To disable OOM exceptions, set RAY_DISABLE_MEMORY_MONITOR=1.\n",
      "---\n",
      "2022-09-16 12:04:58,958\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.sample_with_count()\u001b[39m (pid=18544, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000292F931C0A0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 620, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\memory_monitor.py\", line 162, in raise_if_low_memory\n",
      "    raise RayOutOfMemoryError(\n",
      "ray._private.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node DESKTOP-BKAPO4O is used (7.38 / 7.73 GB). The top 10 memory consumers are:\n",
      "\n",
      "PID\tMEM\tCOMMAND\n",
      "18544\t0.74GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "17572\t0.66GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe -m ipykernel_launcher -f C:\\Users\\Tim\\AppData\\Roaming\\j\n",
      "26756\t0.4GiB\tC:\\Users\\Tim\\AppData\\Local\\Microsoft\\OneDrive\\OneDrive.exe /background\n",
      "1564\t0.31GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "22996\t0.15GiB\tC:\\Program Files\\WindowsApps\\Microsoft.Windows.Photos_2022.30070.26007.0_x64__8wekyb3d8bbwe\\Microsof\n",
      "22716\t0.11GiB\tC:\\Program Files (x86)\\Overwolf\\OverwolfLauncher.exe -overwolfsilent -silent\n",
      "18304\t0.11GiB\tC:\\WINDOWS\\Explorer.EXE\n",
      "18936\t0.1GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --no-startup-window --win-session-start\n",
      "32536\t0.09GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=gpu-process --gpu-preferences=UA\n",
      "26804\t0.09GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "\n",
      "In addition, up to 0.0 GiB of shared memory is currently being used by the Ray object store.\n",
      "---\n",
      "--- Tip: Use the `ray memory` command to list active objects in the cluster.\n",
      "--- To disable OOM exceptions, set RAY_DISABLE_MEMORY_MONITOR=1.\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=5412)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=5412)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5412)\u001b[0m C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=5412)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m 2022-09-16 12:05:11,058\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m [False]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m resetting\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m [False, 0]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m resetting\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -1 at time: 1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -1 at time: 1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -2 at time: 2\n",
      "episode reward mean:  nan\n",
      "End of episode  0\n",
      "Starting episode  1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -2 at time: 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -3 at time: 3\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -3 at time: 3\n",
      "episode reward mean:  nan\n",
      "End of episode  1\n",
      "Starting episode  2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -4 at time: 4\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -4 at time: 4\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -5 at time: 5\n",
      "episode reward mean:  nan\n",
      "End of episode  2\n",
      "Starting episode  3\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -5 at time: 5\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -6 at time: 6\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -7 at time: 7\n",
      "episode reward mean:  nan\n",
      "End of episode  3\n",
      "Starting episode  4\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -7 at time: 7\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -8 at time: 8\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -8 at time: 8\n",
      "episode reward mean:  nan\n",
      "End of episode  4\n",
      "Starting episode  5\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -9 at time: 9\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -9 at time: 9\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -10 at time: 10\n",
      "episode reward mean:  nan\n",
      "End of episode  5\n",
      "Starting episode  6\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -10 at time: 10\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -11 at time: 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -12 at time: 12\n",
      "episode reward mean:  nan\n",
      "End of episode  6\n",
      "Starting episode  7\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -12 at time: 12\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -13 at time: 13\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -13 at time: 13\n",
      "episode reward mean:  nan\n",
      "End of episode  7\n",
      "Starting episode  8\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -14 at time: 14\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -14 at time: 14\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -15 at time: 15\n",
      "episode reward mean:  nan\n",
      "End of episode  8\n",
      "Starting episode  9\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -15 at time: 15\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -16 at time: 16\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -16 at time: 16\n",
      "episode reward mean:  nan\n",
      "End of episode  9\n",
      "Starting episode  10\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -17 at time: 17\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -18 at time: 18\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -18 at time: 18\n",
      "episode reward mean:  nan\n",
      "End of episode  10\n",
      "Starting episode  11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -19 at time: 19\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -19 at time: 19\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -20 at time: 20\n",
      "episode reward mean:  nan\n",
      "End of episode  11\n",
      "Starting episode  12\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -20 at time: 20\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -21 at time: 21\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -21 at time: 21\n",
      "episode reward mean:  nan\n",
      "End of episode  12\n",
      "Starting episode  13\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -22 at time: 22\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -23 at time: 23\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -23 at time: 23\n",
      "episode reward mean:  nan\n",
      "End of episode  13\n",
      "Starting episode  14\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -24 at time: 24\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -24 at time: 24\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -25 at time: 25\n",
      "episode reward mean:  nan\n",
      "End of episode  14\n",
      "Starting episode  15\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -25 at time: 25\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5412)\u001b[0m total score is -26 at time: 26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting episode \u001b[39m\u001b[38;5;124m\"\u001b[39m, episode)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Perform one iteration of training the policy with PPO\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#print(pretty_print(result))\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode reward mean: \u001b[39m\u001b[38;5;124m\"\u001b[39m, result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_reward_mean\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py:347\u001b[0m, in \u001b[0;36mTrainable.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[0;32m    346\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 347\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:661\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    653\u001b[0m     (\n\u001b[0;32m    654\u001b[0m         results,\n\u001b[0;32m    655\u001b[0m         train_iter_ctx,\n\u001b[0;32m    656\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[0;32m    657\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 661\u001b[0m     results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_parallel_to_training\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:2373\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2371\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[0;32m   2372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_disable_execution_plan_api\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m-> 2373\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2374\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2375\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\dqn\\dqn.py:419\u001b[0m, in \u001b[0;36mDQN.training_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cur_ts \u001b[38;5;241m-\u001b[39m last_update \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_network_update_freq\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    418\u001b[0m     to_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39mlocal_worker()\u001b[38;5;241m.\u001b[39mget_policies_to_train()\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_policy_to_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpid\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mto_update\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[NUM_TARGET_UPDATES] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[LAST_TARGET_UPDATE_TS] \u001b[38;5;241m=\u001b[39m cur_ts\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py:1415\u001b[0m, in \u001b[0;36mRolloutWorker.foreach_policy_to_train\u001b[1;34m(self, func, **kwargs)\u001b[0m\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;129m@DeveloperAPI\u001b[39m\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforeach_policy_to_train\u001b[39m(\n\u001b[0;32m   1395\u001b[0m     \u001b[38;5;28mself\u001b[39m, func: Callable[[Policy, PolicyID, Optional[Any]], T], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1396\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[T]:\n\u001b[0;32m   1397\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m \u001b[38;5;124;03m    Calls the given function with each (policy, policy_id) tuple.\u001b[39;00m\n\u001b[0;32m   1399\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;124;03m        `func([policy, pid, **kwargs])`.\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m   1416\u001b[0m         \u001b[38;5;66;03m# Make sure to only iterate over keys() and not items(). Iterating over\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m         \u001b[38;5;66;03m# items will access policy_map elements even for pids that we do not need,\u001b[39;00m\n\u001b[0;32m   1418\u001b[0m         \u001b[38;5;66;03m# i.e. those that are not in policy_to_train. Access to policy_map elements\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m         \u001b[38;5;66;03m# can cause disk access for policies that were offloaded to disk. Since\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m         \u001b[38;5;66;03m# these policies will be skipped in the for-loop accessing them is\u001b[39;00m\n\u001b[0;32m   1421\u001b[0m         \u001b[38;5;66;03m# unnecessary, making subsequent disk access unnecessary.\u001b[39;00m\n\u001b[0;32m   1422\u001b[0m         func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_map[pid], pid, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m pid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_map\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m   1424\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_policy_to_train(pid, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1425\u001b[0m     ]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py:1422\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;129m@DeveloperAPI\u001b[39m\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforeach_policy_to_train\u001b[39m(\n\u001b[0;32m   1395\u001b[0m     \u001b[38;5;28mself\u001b[39m, func: Callable[[Policy, PolicyID, Optional[Any]], T], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1396\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[T]:\n\u001b[0;32m   1397\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m \u001b[38;5;124;03m    Calls the given function with each (policy, policy_id) tuple.\u001b[39;00m\n\u001b[0;32m   1399\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;124;03m        `func([policy, pid, **kwargs])`.\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m   1416\u001b[0m         \u001b[38;5;66;03m# Make sure to only iterate over keys() and not items(). Iterating over\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m         \u001b[38;5;66;03m# items will access policy_map elements even for pids that we do not need,\u001b[39;00m\n\u001b[0;32m   1418\u001b[0m         \u001b[38;5;66;03m# i.e. those that are not in policy_to_train. Access to policy_map elements\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m         \u001b[38;5;66;03m# can cause disk access for policies that were offloaded to disk. Since\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m         \u001b[38;5;66;03m# these policies will be skipped in the for-loop accessing them is\u001b[39;00m\n\u001b[0;32m   1421\u001b[0m         \u001b[38;5;66;03m# unnecessary, making subsequent disk access unnecessary.\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m pid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_map\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m   1424\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_policy_to_train(pid, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1425\u001b[0m     ]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\dqn\\dqn.py:420\u001b[0m, in \u001b[0;36mDQN.training_step.<locals>.<lambda>\u001b[1;34m(p, pid)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cur_ts \u001b[38;5;241m-\u001b[39m last_update \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_network_update_freq\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    418\u001b[0m     to_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39mlocal_worker()\u001b[38;5;241m.\u001b[39mget_policies_to_train()\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39mlocal_worker()\u001b[38;5;241m.\u001b[39mforeach_policy_to_train(\n\u001b[1;32m--> 420\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m p, pid: pid \u001b[38;5;129;01min\u001b[39;00m to_update \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    421\u001b[0m     )\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[NUM_TARGET_UPDATES] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[LAST_TARGET_UPDATE_TS] \u001b[38;5;241m=\u001b[39m cur_ts\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\sac\\sac_tf_policy.py:726\u001b[0m, in \u001b[0;36mTargetNetworkMixin.update_target\u001b[1;34m(self, tau)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_target\u001b[39m(\u001b[38;5;28mself\u001b[39m, tau: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 726\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtau\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtau\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\utils\\tf_utils.py:382\u001b[0m, in \u001b[0;36mmake_tf_callable.<locals>.make_wrapper.<locals>.call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    376\u001b[0m feed_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(args_placeholders, tree\u001b[38;5;241m.\u001b[39mflatten(args)))\n\u001b[0;32m    377\u001b[0m tree\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m ph, v: feed_dict\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setitem__\u001b[39m(ph, v),\n\u001b[0;32m    379\u001b[0m     kwargs_placeholders,\n\u001b[0;32m    380\u001b[0m     kwargs,\n\u001b[0;32m    381\u001b[0m )\n\u001b[1;32m--> 382\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43msession_or_none\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbolic_out\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1191\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1194\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1368\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1371\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m   1377\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1380\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1359\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[0;32m   1360\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1361\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1453\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1454\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import ray.rllib.algorithms.sac.sac as sac\n",
    "from ray.rllib.algorithms.sac.sac import SACConfig\n",
    "\n",
    "config = SACConfig().training(gamma=0.9, lr=0.01)\\\n",
    "    .resources(num_gpus=0)\\\n",
    "    .rollouts(num_rollout_workers=1, recreate_failed_workers=True)\n",
    "config.replay_buffer_config['capacity']=100000\n",
    "config.optimization['entropy_learning_rate']=0.05\n",
    "config.env=LightEnvWrapper\n",
    "print(config.to_dict())\n",
    "# Build a Algorithm object from the config and run 1 training iteration.\n",
    "# trainer = config.build(env=LightEnvWrapper)\n",
    "trainer = sac.SAC(config=config)\n",
    "\n",
    "\n",
    "avg_rewards = []\n",
    "num_iterations = []\n",
    "time_spent = []\n",
    "for episode in range(1000):\n",
    "    print(\"Starting episode \", episode)\n",
    "    # Perform one iteration of training the policy with PPO\n",
    "    result = trainer.train()\n",
    "    #print(pretty_print(result))\n",
    "    print(\"episode reward mean: \", result['episode_reward_mean'])\n",
    "    avg_rewards.append(result['episode_reward_mean'])\n",
    "    num_iterations.append(episode)\n",
    "#     if episode % 10 == 0:\n",
    "#         checkpoint = trainer.save()\n",
    "#         print(\"checkpoint saved at\", checkpoint)\n",
    "    print(\"End of episode \", episode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62260c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17bd7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5766f852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "rllib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
