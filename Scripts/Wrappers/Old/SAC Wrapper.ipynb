{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8df2bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random, math\n",
    "import numpy as np\n",
    "import arcade\n",
    "from skimage import data, color\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from PIL import Image\n",
    "\n",
    "        \n",
    "from LightEnvCopy import LightEnv\n",
    "\n",
    "import gym.spaces\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "# Do the math to figure out our screen dimensions\n",
    "SCREEN_WIDTH = 800\n",
    "SCREEN_HEIGHT = 600\n",
    "SCREEN_TITLE = \"Game 1: Let There Be Light!\"\n",
    "\n",
    "# COnvenient data structure to hold information about actions\n",
    "Action = namedtuple('Action', 'name index delta_i delta_j')\n",
    "\n",
    "up = Action('up', 0, -1, 0)    \n",
    "down = Action('down', 1, 1, 0)    \n",
    "left = Action('left', 2, 0, -1)    \n",
    "right = Action('right', 3, 0, 1)    \n",
    "\n",
    "index_to_actions = {}\n",
    "for action in [up, down, left, right]:\n",
    "    index_to_actions[action.index] = action\n",
    "# print(index_to_actions[0].name)\n",
    "str_to_actions = {}\n",
    "for action in [up, down, left, right]:\n",
    "    str_to_actions[action.name] = action\n",
    "#TF End - Adding in actions for action conversion\n",
    "\n",
    "\n",
    "class LightEnvWrapper(gym.Env, LightEnv):\n",
    "    \"\"\"Class that wraps the Lights Environment to make it \n",
    "    compatible with RLLib.\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"rgb_array\", \"state_pixels\"]}\n",
    "    \n",
    "    def __init__(self, config: EnvContext):\n",
    "        super().__init__(SCREEN_WIDTH, SCREEN_HEIGHT, SCREEN_TITLE)\n",
    "        self.torch_collected = False\n",
    "        self.torch_collected_count = []\n",
    "        self.mygame = LightEnv\n",
    "        self.steps_taken = 0\n",
    "        #The action space is a choice of 4 actions: U/D/L/R.\n",
    "        self.action_space = Discrete(4)\n",
    "        \n",
    "        #The observation space is a fixed image of the current game screen\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84,84, 4), dtype=np.uint8)\n",
    "        \n",
    "    def reset(self):\n",
    "        print(\"resetting in wrapper\")\n",
    "        \n",
    "        if self.torch_collected == 1:\n",
    "            print(\"Torch was collected this episode!\")\n",
    "        else:\n",
    "            print(\"Torch was not collected this episode...\")\n",
    "        self.torch_collected_count.append(self.torch_collected)\n",
    "        print(self.torch_collected_count)\n",
    "\n",
    "        self.render(self)\n",
    "        #Resets the state of the environment for a new episode and an initial observation.\n",
    "        obs_mygame = self.mygame.reset(self)\n",
    "        \n",
    "        #Open up the resetted image to verify working correctly.\n",
    "        obs_mygame.show()\n",
    "        \n",
    "        self.mygame.on_draw(self)\n",
    "        #Convert observation to 84x84 resolution and np array for rllib.\n",
    "        obs = self.convert_observations(obs_mygame)\n",
    "        \n",
    "        self.steps_taken = 0\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps_taken += 1\n",
    "        \n",
    "        #Making sure an action is chosen, either: 0, 1, 2, 3.\n",
    "        assert action in [0, 1, 2, 3] #0-up,1-down,2-left,3-right.\n",
    "        \n",
    "        #Convert the numeric action to a keyword: up, down, left, right.\n",
    "        actions_myenv = index_to_actions[action].name #returns a word, one of: up/down/left/right\n",
    "#         print(f\"action taken: {actions_myenv}\")\n",
    "        \n",
    "        #Update the window with on_update()\n",
    "        self.render(self)\n",
    "#         print(\"env rendered\")\n",
    "        #Compute observation extracted from the window (800x600), with reward and done flag.\n",
    "        obs, reward, done, torch_collected, fps_check = self.mygame.step(self,actions_myenv)\n",
    "        if torch_collected == True:\n",
    "            self.torch_collected = 1\n",
    "        else:\n",
    "            self.torch_collected = 0\n",
    "                    \n",
    "        if self.steps_taken % 100 == 0: #33 steps roughly equates to 1 second in game time\n",
    "            print(f\"total score is {self.score} at time: {self.mygame.time_taken_reported(self)}\")\n",
    "            print(f\"FPS is currently: {fps_check}\")\n",
    "            print(f\"steps taken: {self.steps_taken}\")\n",
    "\n",
    "        #Convert observation to 84x84 resolution and np array for rllib.\n",
    "        obs_mygame = self.convert_observations(obs)\n",
    "        \n",
    "        #If the reward has been obtained, reset the environment and start again\n",
    "        if done == True:\n",
    "            print(f\"done is {done}, resetting environment in wrapper.\")\n",
    "            print(f\"steps taken: {self.steps_taken}\")\n",
    "            obs.show()\n",
    "            self.reset()\n",
    "        \n",
    "        return obs_mygame, reward, done, {}\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        random.seed(seed)\n",
    "\n",
    "    def convert_observations(self, obs_mygame): #resizing and converting to array for rllib processing\n",
    "        # We normalize and concatenate observations\n",
    "        obs = obs_mygame\n",
    "        obs_resized = obs.resize((84,84))\n",
    "        obsarray = np.array(obs_resized)\n",
    "        return obsarray\n",
    "    \n",
    "    def render(self, mode='state_pixels'):\n",
    "#         self.mygame.update(self)\n",
    "        self.mygame.on_draw(self)\n",
    "        test = self.mygame.time_taken_reported(self)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd8f4ad",
   "metadata": {},
   "source": [
    "### Now run the rllib script to train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf3d1b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 13:56:30,095\tWARNING deprecation.py:47 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class '__main__.LightEnvWrapper'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 1, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': True, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.9, 'lr': 0.1, 'train_batch_size': 256, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 1, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 100, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'twin_q': True, 'q_model_config': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': None, 'custom_model': None, 'custom_model_config': {}}, 'policy_model_config': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': None, 'custom_model': None, 'custom_model_config': {}}, 'tau': 0.005, 'initial_alpha': 5, 'target_entropy': 'auto', 'n_step': 1, 'replay_buffer_config': {'_enable_replay_buffer_api': True, 'type': 'MultiAgentPrioritizedReplayBuffer', 'capacity': 100000, 'learning_starts': 1500, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'worker_side_prioritization': False}, 'store_buffer_in_checkpoints': False, 'training_intensity': None, 'optimization': {'actor_learning_rate': 0.0003, 'critic_learning_rate': 0.0003, 'entropy_learning_rate': 0.05}, 'grad_clip': None, 'target_network_update_freq': 0, '_deterministic_loss': False, '_use_beta_distribution': False, 'use_state_preprocessor': -1, 'worker_side_prioritization': -1, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=23032)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=23032)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23032)\u001b[0m C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=23032)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m 2022-09-28 13:56:43,447\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m [False]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m resetting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 13:56:52,108\tINFO trainable.py:160 -- Trainable.setup took 22.014 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-09-28 13:56:52,112\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode  0\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m resetting in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m Torch was not collected this episode...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m [False, 0]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m resetting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 13:56:57,608\tERROR algorithm.py:2173 -- Error in training or evaluation attempt! Trying to recover.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 2373, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\dqn\\dqn.py\", line 358, in training_step\n",
      "    new_sample_batch = synchronous_parallel_sample(\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\execution\\rollout_ops.py\", line 100, in synchronous_parallel_sample\n",
      "    sample_batches = ray.get(\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\worker.py\", line 2275, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RayOutOfMemoryError): \u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=23032, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001EABC68DF10>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 620, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\memory_monitor.py\", line 162, in raise_if_low_memory\n",
      "    raise RayOutOfMemoryError(\n",
      "ray._private.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node DESKTOP-BKAPO4O is used (7.35 / 7.73 GB). The top 10 memory consumers are:\n",
      "\n",
      "PID\tMEM\tCOMMAND\n",
      "9316\t0.73GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe -m ipykernel_launcher -f C:\\Users\\Tim\\AppData\\Roaming\\j\n",
      "17236\t0.48GiB\tC:\\Users\\Tim\\AppData\\Local\\Microsoft\\OneDrive\\OneDrive.exe /background\n",
      "23032\t0.38GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "30580\t0.35GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "29420\t0.21GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=gpu-process --gpu-preferences=UA\n",
      "25220\t0.14GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "28092\t0.14GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --no-startup-window /prefetch:5\n",
      "6248\t0.12GiB\tC:\\Program Files\\WindowsApps\\Microsoft.Windows.Photos_2022.30070.26007.0_x64__8wekyb3d8bbwe\\Microsof\n",
      "396\t0.1GiB\tC:\\WINDOWS\\Explorer.EXE\n",
      "1636\t0.1GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --instant-process --dis\n",
      "\n",
      "In addition, up to 0.0 GiB of shared memory is currently being used by the Ray object store.\n",
      "---\n",
      "--- Tip: Use the `ray memory` command to list active objects in the cluster.\n",
      "--- To disable OOM exceptions, set RAY_DISABLE_MEMORY_MONITOR=1.\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -4 at time: 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 60\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 100\n",
      "episode reward mean:  nan\n",
      "End of episode  0\n",
      "Starting episode  1\n",
      "episode reward mean:  nan\n",
      "End of episode  1\n",
      "Starting episode  2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -5 at time: 3\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 72\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 200\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -7 at time: 5\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 71\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 300\n",
      "episode reward mean:  nan\n",
      "End of episode  2\n",
      "Starting episode  3\n",
      "episode reward mean:  nan\n",
      "End of episode  3\n",
      "Starting episode  4\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -9 at time: 7\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 75\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 400\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -10 at time: 8\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 65\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 500\n",
      "episode reward mean:  nan\n",
      "End of episode  4\n",
      "Starting episode  5\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -12 at time: 10\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 76\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 600\n",
      "episode reward mean:  nan\n",
      "End of episode  5\n",
      "Starting episode  6\n",
      "episode reward mean:  nan\n",
      "End of episode  6\n",
      "Starting episode  7\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -14 at time: 12\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 75\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 700\n",
      "episode reward mean:  nan\n",
      "End of episode  7\n",
      "Starting episode  8\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -15 at time: 13\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 83\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 800\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -17 at time: 15\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 69\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 900\n",
      "episode reward mean:  nan\n",
      "End of episode  8\n",
      "Starting episode  9\n",
      "episode reward mean:  nan\n",
      "End of episode  9\n",
      "Starting episode  10\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -19 at time: 17\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 70\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 1000\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -20 at time: 18\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 67\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 1100\n",
      "episode reward mean:  nan\n",
      "End of episode  10\n",
      "Starting episode  11\n",
      "episode reward mean:  nan\n",
      "End of episode  11\n",
      "Starting episode  12\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -22 at time: 20\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 80\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 1200\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -24 at time: 22\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 88\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 1300\n",
      "episode reward mean:  nan\n",
      "End of episode  12\n",
      "Starting episode  13\n",
      "episode reward mean:  nan\n",
      "End of episode  13\n",
      "Starting episode  14\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -25 at time: 23\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 86\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 1400\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -27 at time: 25\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 1500\n",
      "episode reward mean:  nan\n",
      "End of episode  14\n",
      "Starting episode  15\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -31 at time: 27\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 1600\n",
      "episode reward mean:  nan\n",
      "End of episode  15\n",
      "Starting episode  16\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -33 at time: 28\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 1700\n",
      "episode reward mean:  nan\n",
      "End of episode  16\n",
      "Starting episode  17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 14:01:56,284\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.set_weights()\u001b[39m (pid=23032, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001EABC68DF10>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 620, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\_private\\memory_monitor.py\", line 162, in raise_if_low_memory\n",
      "    raise RayOutOfMemoryError(\n",
      "ray._private.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node DESKTOP-BKAPO4O is used (7.4 / 7.73 GB). The top 10 memory consumers are:\n",
      "\n",
      "PID\tMEM\tCOMMAND\n",
      "9316\t0.69GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe -m ipykernel_launcher -f C:\\Users\\Tim\\AppData\\Roaming\\j\n",
      "17236\t0.48GiB\tC:\\Users\\Tim\\AppData\\Local\\Microsoft\\OneDrive\\OneDrive.exe /background\n",
      "23032\t0.43GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "30580\t0.3GiB\tC:\\Users\\Tim\\Anaconda3\\envs\\rllib\\python.exe C:\\Users\\Tim\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\n",
      "29420\t0.23GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=gpu-process --gpu-preferences=UA\n",
      "25220\t0.17GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --display-capture-permi\n",
      "28092\t0.14GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --no-startup-window /prefetch:5\n",
      "1636\t0.12GiB\tC:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe --type=renderer --instant-process --dis\n",
      "6248\t0.1GiB\tC:\\Program Files\\WindowsApps\\Microsoft.Windows.Photos_2022.30070.26007.0_x64__8wekyb3d8bbwe\\Microsof\n",
      "396\t0.1GiB\tC:\\WINDOWS\\Explorer.EXE\n",
      "\n",
      "In addition, up to 0.0 GiB of shared memory is currently being used by the Ray object store.\n",
      "---\n",
      "--- Tip: Use the `ray memory` command to list active objects in the cluster.\n",
      "--- To disable OOM exceptions, set RAY_DISABLE_MEMORY_MONITOR=1.\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -35 at time: 30\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 1800\n",
      "episode reward mean:  nan\n",
      "End of episode  17\n",
      "Starting episode  18\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -37 at time: 32\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 1900\n",
      "episode reward mean:  nan\n",
      "End of episode  18\n",
      "Starting episode  19\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -38 at time: 33\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 2000\n",
      "episode reward mean:  nan\n",
      "End of episode  19\n",
      "Starting episode  20\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -40 at time: 35\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 2100\n",
      "episode reward mean:  nan\n",
      "End of episode  20\n",
      "Starting episode  21\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -42 at time: 37\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 2200\n",
      "episode reward mean:  nan\n",
      "End of episode  21\n",
      "Starting episode  22\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -43 at time: 38\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 2300\n",
      "episode reward mean:  nan\n",
      "End of episode  22\n",
      "Starting episode  23\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -45 at time: 40\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 2400\n",
      "episode reward mean:  nan\n",
      "End of episode  23\n",
      "Starting episode  24\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -47 at time: 42\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 2500\n",
      "episode reward mean:  nan\n",
      "End of episode  24\n",
      "Starting episode  25\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -48 at time: 43\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 2600\n",
      "episode reward mean:  nan\n",
      "End of episode  25\n",
      "Starting episode  26\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -50 at time: 45\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 2700\n",
      "episode reward mean:  nan\n",
      "End of episode  26\n",
      "Starting episode  27\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -52 at time: 47\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 2800\n",
      "episode reward mean:  nan\n",
      "End of episode  27\n",
      "Starting episode  28\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -53 at time: 48\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 2900\n",
      "episode reward mean:  nan\n",
      "End of episode  28\n",
      "Starting episode  29\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -55 at time: 50\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 3000\n",
      "episode reward mean:  nan\n",
      "End of episode  29\n",
      "Starting episode  30\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m total score is -57 at time: 52\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m FPS is currently: 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23032)\u001b[0m steps taken: 3100\n",
      "episode reward mean:  nan\n",
      "End of episode  30\n",
      "Starting episode  31\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting episode \u001b[39m\u001b[38;5;124m\"\u001b[39m, episode)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Perform one iteration of training the policy with PPO\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#print(pretty_print(result))\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode reward mean: \u001b[39m\u001b[38;5;124m\"\u001b[39m, result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_reward_mean\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py:347\u001b[0m, in \u001b[0;36mTrainable.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[0;32m    346\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 347\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:661\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    653\u001b[0m     (\n\u001b[0;32m    654\u001b[0m         results,\n\u001b[0;32m    655\u001b[0m         train_iter_ctx,\n\u001b[0;32m    656\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[0;32m    657\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 661\u001b[0m     results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_parallel_to_training\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:2378\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2376\u001b[0m         \u001b[38;5;66;03m# In case of any failures, try to ignore/recover the failed workers.\u001b[39;00m\n\u001b[0;32m   2377\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2378\u001b[0m             num_recreated \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtry_recover_from_step_attempt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2379\u001b[0m \u001b[43m                \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2380\u001b[0m \u001b[43m                \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2381\u001b[0m \u001b[43m                \u001b[49m\u001b[43mignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mignore_worker_failures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2382\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrecreate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecreate_failed_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2383\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2384\u001b[0m     results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_recreated_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_recreated\n\u001b[0;32m   2386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results, train_iter_ctx\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:2190\u001b[0m, in \u001b[0;36mAlgorithm.try_recover_from_step_attempt\u001b[1;34m(self, error, worker_set, ignore, recreate)\u001b[0m\n\u001b[0;32m   2186\u001b[0m \u001b[38;5;66;03m# Any other exception.\u001b[39;00m\n\u001b[0;32m   2187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2188\u001b[0m     \u001b[38;5;66;03m# Allow logs messages to propagate.\u001b[39;00m\n\u001b[0;32m   2189\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m-> 2190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m   2192\u001b[0m removed_workers, new_workers \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m   2193\u001b[0m \u001b[38;5;66;03m# Search for failed workers and try to recover (restart) them.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:2373\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2371\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[0;32m   2372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_disable_execution_plan_api\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m-> 2373\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2374\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2375\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\dqn\\dqn.py:403\u001b[0m, in \u001b[0;36mDQN.training_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    400\u001b[0m     train_results \u001b[38;5;241m=\u001b[39m multi_gpu_train_one_step(\u001b[38;5;28mself\u001b[39m, train_batch)\n\u001b[0;32m    402\u001b[0m \u001b[38;5;66;03m# Update replay buffer priorities.\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m \u001b[43mupdate_priorities_in_replay_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_replay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;66;03m# Update target network every `target_network_update_freq` sample steps.\u001b[39;00m\n\u001b[0;32m    411\u001b[0m cur_ts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[\n\u001b[0;32m    412\u001b[0m     NUM_AGENT_STEPS_SAMPLED\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_by_agent_steps\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m NUM_ENV_STEPS_SAMPLED\n\u001b[0;32m    415\u001b[0m ]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\utils\\replay_buffers\\utils.py:97\u001b[0m, in \u001b[0;36mupdate_priorities_in_replay_buffer\u001b[1;34m(replay_buffer, config, train_batch, train_results)\u001b[0m\n\u001b[0;32m     93\u001b[0m     prio_dict[policy_id] \u001b[38;5;241m=\u001b[39m (batch_indices, td_error)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Make the actual buffer API call to update the priority weights on all\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# policies.\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m \u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_priorities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprio_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\utils\\replay_buffers\\multi_agent_prioritized_replay_buffer.py:257\u001b[0m, in \u001b[0;36mMultiAgentPrioritizedReplayBuffer.update_priorities\u001b[1;34m(self, prio_dict)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m policy_id, (batch_indexes, td_errors) \u001b[38;5;129;01min\u001b[39;00m prio_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    256\u001b[0m     new_priorities \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(td_errors) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprioritized_replay_eps\n\u001b[1;32m--> 257\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpolicy_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_priorities\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_indexes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_priorities\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\utils\\replay_buffers\\prioritized_replay_buffer.py:183\u001b[0m, in \u001b[0;36mPrioritizedReplayBuffer.update_priorities\u001b[1;34m(self, idxes, priorities)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(idxes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(priorities)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, priority \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(idxes, priorities):\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m priority \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage)\n\u001b[0;32m    185\u001b[0m     delta \u001b[38;5;241m=\u001b[39m priority \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_it_sum[idx]\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import ray.rllib.algorithms.sac.sac as sac\n",
    "from ray.rllib.algorithms.sac.sac import SACConfig\n",
    "\n",
    "config = SACConfig().training(gamma=0.9, lr=0.1, initial_alpha=5)\\\n",
    "    .resources(num_gpus=0)\\\n",
    "    .rollouts(num_rollout_workers=1, recreate_failed_workers=True)\n",
    "\n",
    "config.replay_buffer_config['capacity']=100000\n",
    "# config.replay_buffer_config['learning_starts']=2500\n",
    "config.optimization['entropy_learning_rate']=0.05\n",
    "\n",
    "config.env=LightEnvWrapper\n",
    "print(config.to_dict())\n",
    "\n",
    "RAY_DISABLE_MEMORY_MONITOR=1\n",
    "\n",
    "# Build a Algorithm object from the config and run 1 training iteration.\n",
    "# trainer = config.build(env=LightEnvWrapper)\n",
    "trainer = sac.SAC(config=config)\n",
    "\n",
    "\n",
    "avg_rewards = []\n",
    "num_iterations = []\n",
    "time_spent = []\n",
    "for episode in range(100):\n",
    "    print(\"Starting episode \", episode)\n",
    "    # Perform one iteration of training the policy with SAC\n",
    "    result = trainer.train()\n",
    "    #print(pretty_print(result))\n",
    "    print(\"episode reward mean: \", result['episode_reward_mean'])\n",
    "    avg_rewards.append(result['episode_reward_mean'])\n",
    "    num_iterations.append(episode)\n",
    "#     if episode % 10 == 0:\n",
    "#         checkpoint = trainer.save()\n",
    "#         print(\"checkpoint saved at\", checkpoint)\n",
    "    print(\"End of episode \", episode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62260c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17bd7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5766f852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "rllib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
